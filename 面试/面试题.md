<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [面试题](#%E9%9D%A2%E8%AF%95%E9%A2%98)
  - [java基础](#java%E5%9F%BA%E7%A1%80)
    - [String StringBuilder StringBuffer 有什么区别？](#string-stringbuilder-stringbuffer-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB)
    - [为什么String不可变](#%E4%B8%BA%E4%BB%80%E4%B9%88string%E4%B8%8D%E5%8F%AF%E5%8F%98)
    - [为什么StringBuilder可变，线程不安全，StringBuffer可变，线程安全](#%E4%B8%BA%E4%BB%80%E4%B9%88stringbuilder%E5%8F%AF%E5%8F%98%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8stringbuffer%E5%8F%AF%E5%8F%98%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8)
    - [既然StringBuilder线程不安全，为什么一般还是用StringBuilder](#%E6%97%A2%E7%84%B6stringbuilder%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%80%E8%88%AC%E8%BF%98%E6%98%AF%E7%94%A8stringbuilder)
    - [为什么重写equals方法时必须重写hashCode方法？](#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E5%86%99equals%E6%96%B9%E6%B3%95%E6%97%B6%E5%BF%85%E9%A1%BB%E9%87%8D%E5%86%99hashcode%E6%96%B9%E6%B3%95)
    - [重写equals需要遵守的四大特性](#%E9%87%8D%E5%86%99equals%E9%9C%80%E8%A6%81%E9%81%B5%E5%AE%88%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7)
    - [并发事务带来的四个问题？](#%E5%B9%B6%E5%8F%91%E4%BA%8B%E5%8A%A1%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%9B%9B%E4%B8%AA%E9%97%AE%E9%A2%98)
    - [实现多线程的方式](#%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F)
    - [线程池](#%E7%BA%BF%E7%A8%8B%E6%B1%A0)
    - [线程池的处理流程](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B)
    - [线程池参数设计原则](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99)
    - [创建线程的方式](#%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F)
    - [线程池如何保证核心线程数量不变 ？待完善](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%A0%B8%E5%BF%83%E7%BA%BF%E7%A8%8B%E6%95%B0%E9%87%8F%E4%B8%8D%E5%8F%98-%E5%BE%85%E5%AE%8C%E5%96%84)
    - [正射](#%E6%AD%A3%E5%B0%84)
    - [反射](#%E5%8F%8D%E5%B0%84)
    - [集合的原理](#%E9%9B%86%E5%90%88%E7%9A%84%E5%8E%9F%E7%90%86)
    - [ArrayList](#arraylist)
    - [HashMap](#hashmap)
    - [ConcurrentHashMap](#concurrenthashmap)
    - [HashTable](#hashtable)
    - [LinkedHashMap](#linkedhashmap)
    - [TreeMap](#treemap)
    - [threadlocal](#threadlocal)
    - [ThreadLocal内存泄漏](#threadlocal%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F)
    - [线程安全的hashmap](#%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84hashmap)
    - [volitile为什么不能保证线程安全](#volitile%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E4%BF%9D%E8%AF%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8)
    - [子类继承父类，子类实例化会默认先调用父类无参构造函数，除非子类构造函数中指定了调用的父类的指定构造函数（例如super(name)）](#%E5%AD%90%E7%B1%BB%E7%BB%A7%E6%89%BF%E7%88%B6%E7%B1%BB%E5%AD%90%E7%B1%BB%E5%AE%9E%E4%BE%8B%E5%8C%96%E4%BC%9A%E9%BB%98%E8%AE%A4%E5%85%88%E8%B0%83%E7%94%A8%E7%88%B6%E7%B1%BB%E6%97%A0%E5%8F%82%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E9%99%A4%E9%9D%9E%E5%AD%90%E7%B1%BB%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E4%B8%AD%E6%8C%87%E5%AE%9A%E4%BA%86%E8%B0%83%E7%94%A8%E7%9A%84%E7%88%B6%E7%B1%BB%E7%9A%84%E6%8C%87%E5%AE%9A%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E4%BE%8B%E5%A6%82supername)
  - [java基础-锁](#java%E5%9F%BA%E7%A1%80-%E9%94%81)
    - [并发编程的三个概念](#%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%89%E4%B8%AA%E6%A6%82%E5%BF%B5)
    - [锁有哪几种](#%E9%94%81%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D)
    - [synchronized](#synchronized)
    - [ReentrantLock,synchronized,volatile区别](#reentrantlocksynchronizedvolatile%E5%8C%BA%E5%88%AB)
    - [volatile防止指令重排原理](#volatile%E9%98%B2%E6%AD%A2%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92%E5%8E%9F%E7%90%86)
    - [ReentrantLock,synchronized,volatile 实现原理](#reentrantlocksynchronizedvolatile-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86)
      - [synchronized 原理](#synchronized-%E5%8E%9F%E7%90%86)
      - [synchronized应用场景](#synchronized%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF)
    - [cas](#cas)
    - [cas的缺点](#cas%E7%9A%84%E7%BC%BA%E7%82%B9)
    - [java中cas的应用](#java%E4%B8%ADcas%E7%9A%84%E5%BA%94%E7%94%A8)
    - [自旋锁](#%E8%87%AA%E6%97%8B%E9%94%81)
    - [自适应自旋锁](#%E8%87%AA%E9%80%82%E5%BA%94%E8%87%AA%E6%97%8B%E9%94%81)
    - [无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁](#%E6%97%A0%E9%94%81-vs-%E5%81%8F%E5%90%91%E9%94%81-vs-%E8%BD%BB%E9%87%8F%E7%BA%A7%E9%94%81-vs-%E9%87%8D%E9%87%8F%E7%BA%A7%E9%94%81)
    - [无锁](#%E6%97%A0%E9%94%81)
    - [偏向锁](#%E5%81%8F%E5%90%91%E9%94%81)
    - [轻量级锁](#%E8%BD%BB%E9%87%8F%E7%BA%A7%E9%94%81)
    - [重量级锁](#%E9%87%8D%E9%87%8F%E7%BA%A7%E9%94%81)
    - [公平锁 VS 非公平锁](#%E5%85%AC%E5%B9%B3%E9%94%81-vs-%E9%9D%9E%E5%85%AC%E5%B9%B3%E9%94%81)
    - [可重入锁 VS 非可重入锁](#%E5%8F%AF%E9%87%8D%E5%85%A5%E9%94%81-vs-%E9%9D%9E%E5%8F%AF%E9%87%8D%E5%85%A5%E9%94%81)
    - [独享锁 VS 共享锁](#%E7%8B%AC%E4%BA%AB%E9%94%81-vs-%E5%85%B1%E4%BA%AB%E9%94%81)
    - [java8对synchronized的优化](#java8%E5%AF%B9synchronized%E7%9A%84%E4%BC%98%E5%8C%96)
    - [JUC包有哪些类](#juc%E5%8C%85%E6%9C%89%E5%93%AA%E4%BA%9B%E7%B1%BB)
    - [aqs解释](#aqs%E8%A7%A3%E9%87%8A)
    - [线程同步的方法](#%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E7%9A%84%E6%96%B9%E6%B3%95)
  - [mysql](#mysql)
    - [数据库事务四大特性？](#%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7)
    - [数据库事务隔离级别（泛指所有数据库，不是特指mysql）](#%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%B3%9B%E6%8C%87%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8D%E6%98%AF%E7%89%B9%E6%8C%87mysql)
    - [Mysql MVCC](#mysql-mvcc)
    - [mysql事务执行过程](#mysql%E4%BA%8B%E5%8A%A1%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B)
    - [为什么项目中mysql隔离级别选择提交读](#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%A1%B9%E7%9B%AE%E4%B8%ADmysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E9%80%89%E6%8B%A9%E6%8F%90%E4%BA%A4%E8%AF%BB)
    - [共享锁，排它锁](#%E5%85%B1%E4%BA%AB%E9%94%81%E6%8E%92%E5%AE%83%E9%94%81)
    - [什么是间隙锁](#%E4%BB%80%E4%B9%88%E6%98%AF%E9%97%B4%E9%9A%99%E9%94%81)
    - [mysql RR隔离级别下会产生幻读吗](#mysql-rr%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E4%B8%8B%E4%BC%9A%E4%BA%A7%E7%94%9F%E5%B9%BB%E8%AF%BB%E5%90%97)
    - [解决幻读的问题](#%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E7%9A%84%E9%97%AE%E9%A2%98)
    - [select加锁分析](#select%E5%8A%A0%E9%94%81%E5%88%86%E6%9E%90)
    - [Redo log, bin log, Undo log](#redo-log-bin-log-undo-log)
    - [读写分离？](#%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB)
    - [主从同步](#%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5)
    - [索引原理](#%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86)
    - [分库分表](#%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8)
    - [myisam为什么比innodb更适合读频繁，很少写的场景](#myisam%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AF%94innodb%E6%9B%B4%E9%80%82%E5%90%88%E8%AF%BB%E9%A2%91%E7%B9%81%E5%BE%88%E5%B0%91%E5%86%99%E7%9A%84%E5%9C%BA%E6%99%AF)
    - [mysql选择哪一列建索引的依据](#mysql%E9%80%89%E6%8B%A9%E5%93%AA%E4%B8%80%E5%88%97%E5%BB%BA%E7%B4%A2%E5%BC%95%E7%9A%84%E4%BE%9D%E6%8D%AE)
    - [sql的执行流程](#sql%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B)
    - [mysql慢sql问题排查](#mysql%E6%85%A2sql%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5)
    - [undo log是什么？](#undo-log%E6%98%AF%E4%BB%80%E4%B9%88)
    - [change buffer是什么](#change-buffer%E6%98%AF%E4%BB%80%E4%B9%88)
    - [redo log 是什么？](#redo-log-%E6%98%AF%E4%BB%80%E4%B9%88)
    - [InnoDB事务实现原理](#innodb%E4%BA%8B%E5%8A%A1%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86)
    - [innodb和myisam的区别](#innodb%E5%92%8Cmyisam%E7%9A%84%E5%8C%BA%E5%88%AB)
    - [缓解数据库压力的办法](#%E7%BC%93%E8%A7%A3%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%8B%E5%8A%9B%E7%9A%84%E5%8A%9E%E6%B3%95)
    - [聚簇索引和非聚簇索引区别](#%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E5%92%8C%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E5%8C%BA%E5%88%AB)
    - [聚簇索引的好处](#%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E7%9A%84%E5%A5%BD%E5%A4%84)
    - [聚簇索引劣势](#%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E5%8A%A3%E5%8A%BF)
    - [sql：while col1 like '%key' 怎么用索引](#sqlwhile-col1-like-%25key-%E6%80%8E%E4%B9%88%E7%94%A8%E7%B4%A2%E5%BC%95)
    - [explain除了看索引还看什么](#explain%E9%99%A4%E4%BA%86%E7%9C%8B%E7%B4%A2%E5%BC%95%E8%BF%98%E7%9C%8B%E4%BB%80%E4%B9%88)
    - [mysql分库分表 平滑扩容方案](#mysql%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8-%E5%B9%B3%E6%BB%91%E6%89%A9%E5%AE%B9%E6%96%B9%E6%A1%88)
    - [数据库平滑扩容的方案](#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B9%B3%E6%BB%91%E6%89%A9%E5%AE%B9%E7%9A%84%E6%96%B9%E6%A1%88)
  - [elasticsearch](#elasticsearch)
    - [Elasticsearch是什么？](#elasticsearch%E6%98%AF%E4%BB%80%E4%B9%88)
    - [为什么使用elasticsearch](#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8elasticsearch)
    - [es的架构原理 todo](#es%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86-todo)
    - [什么是倒排索引](#%E4%BB%80%E4%B9%88%E6%98%AF%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95)
    - [默认分词器](#%E9%BB%98%E8%AE%A4%E5%88%86%E8%AF%8D%E5%99%A8)
    - [es的query大量数据报错的问题](#es%E7%9A%84query%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE%E6%8A%A5%E9%94%99%E7%9A%84%E9%97%AE%E9%A2%98)
    - [es深度分页解决方案  scrollid会变吗](#es%E6%B7%B1%E5%BA%A6%E5%88%86%E9%A1%B5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88--scrollid%E4%BC%9A%E5%8F%98%E5%90%97)
    - [es调优，慢查询怎么处理](#es%E8%B0%83%E4%BC%98%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86)
    - [es相比于mysql的区别](#es%E7%9B%B8%E6%AF%94%E4%BA%8Emysql%E7%9A%84%E5%8C%BA%E5%88%AB)
    - [es查询、更新、删除过程。](#es%E6%9F%A5%E8%AF%A2%E6%9B%B4%E6%96%B0%E5%88%A0%E9%99%A4%E8%BF%87%E7%A8%8B)
  - [中间件](#%E4%B8%AD%E9%97%B4%E4%BB%B6)
  - [mq消息队列](#mq%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97)
    - [优缺点](#%E4%BC%98%E7%BC%BA%E7%82%B9)
    - [目前主流mq类型](#%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81mq%E7%B1%BB%E5%9E%8B)
    - [rabbitmq kafka rocketmq区别](#rabbitmq-kafka-rocketmq%E5%8C%BA%E5%88%AB)
    - [rabbitmq原理](#rabbitmq%E5%8E%9F%E7%90%86)
    - [rabbitmq的exchange路由规则](#rabbitmq%E7%9A%84exchange%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99)
    - [rabbitmq消息丢失处理策略：](#rabbitmq%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E5%A4%84%E7%90%86%E7%AD%96%E7%95%A5)
    - [假如是ack在网络中丢失，发送方或者exchange重新发送消息，接收方重复消费怎么办？](#%E5%81%87%E5%A6%82%E6%98%AFack%E5%9C%A8%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%B8%A2%E5%A4%B1%E5%8F%91%E9%80%81%E6%96%B9%E6%88%96%E8%80%85exchange%E9%87%8D%E6%96%B0%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%8E%A5%E6%94%B6%E6%96%B9%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E6%80%8E%E4%B9%88%E5%8A%9E)
    - [mq消息的顺序性](#mq%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7)
  - [spring相关](#spring%E7%9B%B8%E5%85%B3)
    - [Spring中的设计模式](#spring%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F)
    - [spring 启动过程](#spring-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B)
    - [spring事务](#spring%E4%BA%8B%E5%8A%A1)
    - [事务作用](#%E4%BA%8B%E5%8A%A1%E4%BD%9C%E7%94%A8)
    - [编程式事务](#%E7%BC%96%E7%A8%8B%E5%BC%8F%E4%BA%8B%E5%8A%A1)
    - [声明式事务](#%E5%A3%B0%E6%98%8E%E5%BC%8F%E4%BA%8B%E5%8A%A1)
    - [基于@Transactional注解](#%E5%9F%BA%E4%BA%8Etransactional%E6%B3%A8%E8%A7%A3)
    - [传播级别](#%E4%BC%A0%E6%92%AD%E7%BA%A7%E5%88%AB)
    - [隔离级别](#%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB)
    - [spring事务失效的情况](#spring%E4%BA%8B%E5%8A%A1%E5%A4%B1%E6%95%88%E7%9A%84%E6%83%85%E5%86%B5)
    - [spring的事务隔离级别和mysql的事务隔离级别不一致会这么样？](#spring%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E4%B8%8D%E4%B8%80%E8%87%B4%E4%BC%9A%E8%BF%99%E4%B9%88%E6%A0%B7)
    - [spring如何解决循环依赖](#spring%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96)
    - [产生循环依赖的代码](#%E4%BA%A7%E7%94%9F%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E7%9A%84%E4%BB%A3%E7%A0%81)
    - [解决循环依赖](#%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96)
    - [java怎么解决循环依赖](#java%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96)
    - [@autowired原理](#autowired%E5%8E%9F%E7%90%86)
    - [spring自动注入](#spring%E8%87%AA%E5%8A%A8%E6%B3%A8%E5%85%A5)
    - [spring bean初始化流程](#spring-bean%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B)
    - [bean生命周期](#bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F)
    - [springboot启动过程](#springboot%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B)
  - [其他知识](#%E5%85%B6%E4%BB%96%E7%9F%A5%E8%AF%86)
    - [接口高并发保护系统的方案](#%E6%8E%A5%E5%8F%A3%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BF%9D%E6%8A%A4%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%96%B9%E6%A1%88)
    - [限流](#%E9%99%90%E6%B5%81)
    - [助力时网络故障，同一个人对同一个团助力两次](#%E5%8A%A9%E5%8A%9B%E6%97%B6%E7%BD%91%E7%BB%9C%E6%95%85%E9%9A%9C%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%AF%B9%E5%90%8C%E4%B8%80%E4%B8%AA%E5%9B%A2%E5%8A%A9%E5%8A%9B%E4%B8%A4%E6%AC%A1)
    - [更新开团表商品剩余价值时，并发执行问题](#%E6%9B%B4%E6%96%B0%E5%BC%80%E5%9B%A2%E8%A1%A8%E5%95%86%E5%93%81%E5%89%A9%E4%BD%99%E4%BB%B7%E5%80%BC%E6%97%B6%E5%B9%B6%E5%8F%91%E6%89%A7%E8%A1%8C%E9%97%AE%E9%A2%98)
    - [接口反应时间长的问题排查（接口慢）](#%E6%8E%A5%E5%8F%A3%E5%8F%8D%E5%BA%94%E6%97%B6%E9%97%B4%E9%95%BF%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8E%A5%E5%8F%A3%E6%85%A2)
    - [cpu100%排查 ? 待完善](#cpu100%25%E6%8E%92%E6%9F%A5--%E5%BE%85%E5%AE%8C%E5%96%84)
  - [redis](#redis)
    - [redis介绍](#redis%E4%BB%8B%E7%BB%8D)
    - [redis应用场景](#redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF)
    - [redis数据类型](#redis%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B)
    - [redis hash](#redis-hash)
    - [渐进式hash小结](#%E6%B8%90%E8%BF%9B%E5%BC%8Fhash%E5%B0%8F%E7%BB%93)
    - [rehash的其他细节和缺点](#rehash%E7%9A%84%E5%85%B6%E4%BB%96%E7%BB%86%E8%8A%82%E5%92%8C%E7%BC%BA%E7%82%B9)
    - [渐进式rehash带来的问题](#%E6%B8%90%E8%BF%9B%E5%BC%8Frehash%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98)
    - [redis主从同步](#redis%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5)
    - [单线程的redis为什么这么快](#%E5%8D%95%E7%BA%BF%E7%A8%8B%E7%9A%84redis%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB)
    - [redis为什么是单线程的](#redis%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%95%E7%BA%BF%E7%A8%8B%E7%9A%84)
    - [redis的四个问题](#redis%E7%9A%84%E5%9B%9B%E4%B8%AA%E9%97%AE%E9%A2%98)
    - [redis删除策略](#redis%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5)
    - [redis内存淘汰策略](#redis%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5)
    - [缓存穿透](#%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F)
    - [缓存雪崩](#%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9)
    - [缓存击穿](#%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF)
    - [缓存并发竞争问题](#%E7%BC%93%E5%AD%98%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E9%97%AE%E9%A2%98)
    - [redis分布式锁是怎么实现的](#redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84)
    - [redis zset原理](#redis-zset%E5%8E%9F%E7%90%86)
    - [超时解锁导致并发](#%E8%B6%85%E6%97%B6%E8%A7%A3%E9%94%81%E5%AF%BC%E8%87%B4%E5%B9%B6%E5%8F%91)
    - [锁误解除](#%E9%94%81%E8%AF%AF%E8%A7%A3%E9%99%A4)
    - [redis主从复制](#redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6)
    - [redis缓存数据库双写一致性](#redis%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7)
    - [redis需要掌握的](#redis%E9%9C%80%E8%A6%81%E6%8E%8C%E6%8F%A1%E7%9A%84)
  - [待分类](#%E5%BE%85%E5%88%86%E7%B1%BB)
    - [epoll原理 //todo](#epoll%E5%8E%9F%E7%90%86-todo)
    - [java接口架构图](#java%E6%8E%A5%E5%8F%A3%E6%9E%B6%E6%9E%84%E5%9B%BE)
    - [fork join  // todo](#fork-join---todo)
  - [mybatis](#mybatis)
    - [mybatis 原理 // todo](#mybatis-%E5%8E%9F%E7%90%86--todo)
    - [mybatis架构图](#mybatis%E6%9E%B6%E6%9E%84%E5%9B%BE)
    - [mybatis缓存](#mybatis%E7%BC%93%E5%AD%98)
    - [一级缓存](#%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98)
    - [二级缓存](#%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98)
  - [jvm](#jvm)
    - [g1垃圾收集器，强软弱虚引用，gc算法](#g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E5%BC%BA%E8%BD%AF%E5%BC%B1%E8%99%9A%E5%BC%95%E7%94%A8gc%E7%AE%97%E6%B3%95)
    - [内存泄漏怎么排查问题](#%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%80%8E%E4%B9%88%E6%8E%92%E6%9F%A5%E9%97%AE%E9%A2%98)
    - [mybatis怎么写 like查询，即不同于普通的like查询](#mybatis%E6%80%8E%E4%B9%88%E5%86%99-like%E6%9F%A5%E8%AF%A2%E5%8D%B3%E4%B8%8D%E5%90%8C%E4%BA%8E%E6%99%AE%E9%80%9A%E7%9A%84like%E6%9F%A5%E8%AF%A2)
    - [jvm类加载机制 过程](#jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6-%E8%BF%87%E7%A8%8B)
    - [jvm内存模型](#jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B)
  - [todo](#todo)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


# 面试题

## java基础

### String StringBuilder StringBuffer 有什么区别？
String不可变，StringBuilder可变，线程不安全，StringBuffer可变，线程安全

### 为什么String不可变
<https://www.cnblogs.com/wkfvawl/p/11693260.html>

### 为什么StringBuilder可变，线程不安全，StringBuffer可变，线程安全
见博客
<https://juejin.cn/post/6844903923908788237>
<https://blog.csdn.net/codingisforlife/article/details/100166513>

### 既然StringBuilder线程不安全，为什么一般还是用StringBuilder
方法内的局部变量，不存在线程安全的问题，肯定StringBuilder。
什么是线程安全。核心点其实很简单，多线程必须访问同一个对象才有可能产生线程安全的问题。
在一个方法内部，声明了一个对象，只要你没有在这个方法里面在创建线程使用这个对象，人为构建多线程的环境，那么这都不是多线程，进而也没有线程安全问题了。
但如果类似于类的实例变量（在controller里面定义一个全局变量）这样，由于spring MVC的机制（controller是单例，tomcat容器接收请求后是多线程响应），每个controller的方法都可能会被多个线程调用，这个时候，类的实例变量就存在线程安全问题了，所以应该用线程安全的类。

### 为什么重写equals方法时必须重写hashCode方法？
equals方法是超类Object中的一个基本方法，用于检测一个对象是否与另外一个对象相等。默认是比较引用值。
hashCode方法的意思就是散列码，也就是哈希码，返回的是将对象的地址值映射为integer类型的哈希值
hashCode方法中有约定：“相同对象必须有相同哈希值” (但是有相同hashCode值的对象不一定相等)
为什么有这个约定以及需要重写主要因为set，map集合的原理。
Set集合中元素是无序不可重复的。因此插入数据时会先比较两个元素hashCode值是否一致，不一致则集合中没有重复元素，插入。hashCode一致的话再调用equals方法比较两个元素是否一致。
（为什么不直接比较equals方法呢，因为只用equals方法的话，新插入一个元素需要和set集合内所有元素比较，费时。先获取hashcode值再取余的方法得到元素应该存储在set集合中的位置，再和指定位置上的元素比较equals，效率高）
因此假如只重写User类的equals方法，重写为比较User对象的所有属性值是否相等。不重写User类的hashCode方法。
set集合中已有User(name="david",age=18)的对象user1，再new一个属性相同的对象后，因为hashCode值和user1不同，计算在set集合数组中位置也不同，最后成功插入到set集合中。违法了set集合中不能有相同对象的约定。

参考<https://juejin.cn/post/6844903854639693837>
<https://blog.csdn.net/u013679744/article/details/57074669>

### 重写equals需要遵守的四大特性
参考<https://blog.csdn.net/javazejian/article/details/51348320>

### 并发事务带来的四个问题？

### 实现多线程的方式
1. 继承Thread类，重写run方法 （无返回值）

2. 实现Runnable接口，重写run方法，实现Runnable接口的实现类的实例对象作为Thread构造函数的target （无返回值）

3. 通过Callable和FutureTask创建线程 （有返回值）

4. 通过线程池创建线程 （有返回值）(new ThreadPoolExecutor(), Executors.newFixedThreadPool 等等)

详细参考<https://zhuanlan.zhihu.com/p/47401636>

### 线程池

### 线程池的处理流程
1. 提交任务后，线程池先判断线程数是否达到了核心线程数（corePoolSize）。如果未达到线程数，则创建核心线程处理任务；否则，就执行下一步；
2. 接着线程池判断任务队列是否满了。如果没满，则将任务添加到任务队列中；否则，执行下一步；
3. 接着因为任务队列满了，线程池就判断线程数是否达到了最大线程数。如果未达到，则创建非核心线程处理任务；
4. 否则，就执行饱和策略，默认会抛出RejectedExecutionException异常。

### 线程池参数设计原则
1. corePoolSize = 80%情况下每秒任务数 * 每个任务执行时间 = 200 * 0.1 = 20
2. queueCapacity = (coreSizePool/taskcost) * responsetime=20/0.1 * 1 = 200;    // responsetime:系统允许容忍的最大响应时间，假设为1秒，即一个任务进来需要在1秒内被处理响应
3. maxPoolSize = (max(tasks)- queueCapacity)/ * taskcost = (800-200)/ * 0.1 = 60
4. keepAliveTime 用默认值 60s
5. 丢弃策略 用默认抛出异

// todo 同时还要考虑cpu的线程数
如果是cpu密集型任务，cpu线程数作为线程池最大线程数
如果是io密集型任务，最大线程数设置为cpu核心数的两倍


### 线程池如何保证核心线程数量不变 ？待完善
线程池当未调用 shutdown 方法时，是通过队列的 take 方法阻塞核心线程（Worker）的 run 方法从而保证核心线程不被销毁的。
核心线程数未超过 corePoolSize，每添加新的任务（command），都会创建新的线程（Worker中创建），即使有空闲线程存在；
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_6.png)
<https://blog.csdn.net/smile_from_2015/article/details/105259789>

### 线程池里的线程，执行任务有异常，线程会停止吗
不会，不管报不报错，线程只要不超过核心线程池数量，都会存活。
超过核心线程数的线程，也不会立马停止，会经过 executor.setKeepAliveSeconds(60); 这个设置的时间之后销毁


### 正射
```
Apple apple = new Apple(); //直接初始化，「正射」
apple.setPrice(4);
```
### 反射
```
Class clz = Class.forName("com.chenshuyi.reflect.Apple");
Method method = clz.getMethod("setPrice", int.class);
Constructor constructor = clz.getConstructor();
Object object = constructor.newInstance();
method.invoke(object, 4);
```
上面两段代码的执行结果，其实是完全一样的。
参考<https://www.cnblogs.com/chanshuyi/p/head_first_of_reflection.html>

### 反射在spring中的应用
1. IOC 容器中创建 Bean
2.  调用方法（比如 AOP 切面调用原始方法）
3. Spring 会扫描类上的注解，比如 @Service、@Autowired，然后反射读取它们的元数据：
4. 代理对象生成（JDK 动态代理、CGLIB）（失误管理）
5. SpringBoot 自动配置（@ConfigurationProperties）

### 集合的原理
1. ArrayList
2. HashMap   (数组+链表+红黑树 线程不安全)
3. ConcurrentHashMap（线程安全）
3. HashTable   （线程安全 不为bull）
3. LinkedHashMap  （插入有序的hashmap）
4. TreeMap   （按键排序,底层是红黑树结构）



### ArrayList
底层是数组。允许空值和重复值。默认初始化为空数组，第一次add后初始化为10。当空间用完，其会按照原数组空间的 1.5 倍进行扩容。即10->15->22 。
### HashMap
参考<https://tech.meituan.com/2016/06/24/java-hashmap.html>
底层是数组+链表+红黑树。初始化后，第一次put会生成大小为16的数组。负载因子默认为0.75。put时，计算hashcode，在数组中的位置，key相同则替换value，不相同则在后面生成链表放入值。链表长度大于8则链表转换成红黑树，小于6时再转回链表。最后检测已存储数量是否为数组的0.75阈值，超过则resize扩容，扩容成原来的两倍
*为什么初始化是2的幂次方，扩容后也是2的幂次方*
为了提高哈希定位的效率,  hash%length，但是效率低，用(n - 1) & hash，二进制比较的速度快，n是数组长度，前提是n是2的幂次方
*链表为啥从一开始的尾插法改为了头插法*
为了效率更高，尾插要遍历整个链表
*为什么转换负载因子是0.75*
答：// todo 经验所致，这样存取效率最高
*为什么转换成红黑树阈值是8*
答：// todo 转换红黑树费时，太小转换不划算，太多转换，则链表存取费时，8大概是红黑树4层
*为什么红黑树转链表阈值是6而不是8*
答：因为避免插入删除时频繁转换
### ConcurrentHashMap
jdk1.7采用Segment分段锁。1.8利用CAS+Synchronized来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构。

1. 线程安全的，1.7之前是分段锁，1.8是cas+synchronized
   put操作分两种情况，
1. 当前hash表对应当前key的index上没有元素时
   cas的方式创建node放入数组
   2.当前hash表对应当前key的index上已经存在元素时(hash碰撞)
   synchronized对当前位置的节点头 加锁
> https://juejin.im/post/6844903813892014087
// todo
### HashTable
// todo
### LinkedHashMap
// todo
### TreeMap
// todo


### threadlocal
定义: 可以创建线程私有变量
作用:    1. 实现线程安全
2. 实现单个线程单例以及单个线程上下文信息存储，比如交易id等
3. 承载一些线程相关的数据，避免在方法中来回传递参数
原理
threadlocal.set()
就是往threadlocalMap里set东西
ThreadLocalMap是ThreadLocal的内部类
map中key是当前ThreadLocal对象
Thread为每个线程维护了ThreadLocalMap这么一个Map，而ThreadLocalMap的key是LocalThread对象本身，value则是要存储的对象
每个Thread维护着一个ThreadLocalMap的引用
   
参考<https://juejin.cn/post/7224051468941049916>
<https://cloud.tencent.com/developer/article/1636025>


### ThreadLocal内存泄漏
原因：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏
可是ThreadLocal并不会产生内存泄露，因为ThreadLocalMap在选择key的时候，并不是直接选择ThreadLocal实例，而是ThreadLocal实例的弱引用。
map里是弱引用，只要gc就会回收，不会发生内存泄漏

记得  threadLocal.remove();  // 清理线程副本 不然可能内存泄漏 

key弱引用可能被gc回收，但是value可能是对象是强引用，还是可能造成内存泄漏

### ThreadLocal具体应用场景
####  1.保存登录用户信息 
我希望在多个方法中都能拿到当前用户信息。
```
public class UserContextHolder {
    private static final ThreadLocal<UserInfo> userThreadLocal = new ThreadLocal<>();
    public static void set(UserInfo userInfo) {
        userThreadLocal.set(userInfo);
    }
    public static UserInfo get() {
        return userThreadLocal.get();
    }
    public static void clear() {
        userThreadLocal.remove();
    }
}
```
登录成功后设置  UserContextHolder.set(new UserInfo(userId, userName));
任何方法里要使用时，String uid = UserContextHolder.get().getUserId();
这样就不用在多个方法直接传递用户信息了

#### 2.日志 traceId 跟踪请求链路
借口请求入口中，设置uuid，后面每次打印都打印uuid，这样就知道这次请求分别打的是哪些日志，方便追踪

#### 3. spring事务连接
#### 3. MyBatis 缓存机制




### 线程安全的hashmap
hashtable  set get方法上加synchronized
synchronizedMap    synchronized获取mutex锁
concurrentHashMap

### volitile为什么不能保证线程安全
<https://www.cnblogs.com/laipimei/p/11857786.html>

### 子类继承父类，子类实例化会默认先调用父类无参构造函数，除非子类构造函数中指定了调用的父类的指定构造函数（例如super(name)）


### java常见的运行时异常
参考<https://blog.csdn.net/A350204530/article/details/79432943>

## java基础-锁

### 并发编程的三个概念
- 原子性
  即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。
  ① y=1;   原子操作
  ② y=x;   非原子操作。两步，取x值再赋值给y
  ③ y++;   非原子操作，三步，相当于y=y+1
- 可见性
  可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。
- 有序性
  即程序执行的顺序按照代码的先后顺序执行。实际jvm执行时会指令重排序，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。（多线程就不一定了）

参考<https://www.jianshu.com/p/94ae2257c747>

### 锁有哪几种

### synchronized
synchronized是java中加锁的关键字，可以用来给对象和方法或者代码块加锁，当它锁定一个方法或者一个代码块的时候，同一时刻最多只有一个线程可以执行这段代码。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。
参考<https://juejin.cn/post/6844903670933356551>
- 修饰实例方法
```
public class AccountingSync implements Runnable{
    static int i=0;
    public synchronized void increase(){
        i++;
    }
    ...
}
```
锁的是这个类的实例对象

- 修饰静态方法
```
public static synchronized void increase(){
    i++;
}
```
- 修饰类
```
synchronized (Singleton.class) {  
}
```
锁对象是当前类的class对象

- 修饰代码块
```
public void run() {
        static AccountingSync instance=new AccountingSync();
        synchronized(instance){
            for(int j=0;j<1000000;j++){
                    i++;
              }
        }
    }
```
锁的是instance对象

ReentrantLock
使用案例<https://www.liaoxuefeng.com/wiki/1252599548343744/1306580960149538>

### ReentrantLock,synchronized,volatile区别
- ReentrantLock
  可保证原子性,可见性。加锁方式同步，阻塞式的同步。
  ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。
  默认创建非公平锁。也可创建公平锁。
  一个ReentrantLock对象可以同时绑定对个对象。
  等待可中断
  可重入


- synchronized
  可保证原子性，可见性。加锁方式同步，阻塞式的同步。
  Synchronized是java语言的关键字，是原生语法层面的互斥，需要jvm实现。
  非公平锁
  只能绑定一个对象
  不能等待可中断
  可重入

- volatile
  可以保证可见性和有序性,不能保证原子性。只能作用于变量
  **可见性**
  使用volatile关键字会强制将修改的值立即写入主存,也就是线程2在修改完stop的值之后会把stop的值立即写入内存；
  使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效,由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。
  **有序性**
  volatile关键字能禁止指令重排序。在volatile变量之前的指令不能在volatile之后执行，在volatile之后的指令也不能在volatile之前执行

### volatile防止指令重排原理
编译后生成 内存屏障指令
内存屏障指令前面的代码不可以越过这个屏障，后面的代码也不能越过这个屏障

### ReentrantLock,synchronized,volatile 实现原理
#### synchronized 原理
参考<https://www.cnblogs.com/aspirant/p/11470858.html>
每个对象都有一个监视器锁（Monitor）
synchronized(obj) 会尝试获取 obj 对象的监视器锁。
synchronized 编译后有两条指令monitorenter，monitorexit。
monitorenter：线程执行monitorenter指令时尝试获取monitor的所有权，过程如下：
1. 如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者；
   2。 如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1；
3. 如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权；
   monitorexit：执行monitorexit的线程必须是objectref所对应的monitor的所有者。指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor，不再是这个monitor的所有者。其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权。

synchronized 修饰实例方法时
常量池中多了 ACC_SYNCHRONIZED 标示符
当方法调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。

两个指令的执行是JVM通过调用操作系统的互斥原语mutex来实现，被阻塞的线程会被挂起、等待重新调度，会导致“用户态和内核态”两个态之间来回切换，对性能有较大影响。

对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充
如图<20210609_1_object.png>
Synchronized用的锁就是存在Java对象头里的

#### synchronized应用场景
com.kedacom.kapsdatacollection.scene.loudi.service.impl.LouDiCollectionServiceImpl#departmentDataCollection
> 现场项目因为token一小时有效，同时token接口请求一天有上限，所以同步获取token

### cas
参考<https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651749434&idx=3&sn=5ffa63ad47fe166f2f1a9f604ed10091&chksm=bd12a5778a652c61509d9e718ab086ff27ad8768586ea9b38c3dcf9e017a8e49bcae3df9bcc8&scene=38#wechat_redirect>
CAS算法涉及到三个操作数：
V：要修改的变量（比如内存里的某个值）
E：期望的值（期望它还没被别人改）
N：要更新的新值
如果 V == E，就把 V 改成 N；否则什么都不做。


### cas的缺点
1. 循环时间长开销大
2. 只能保证一个共享变量的原子操作
3. ABA问题

### java中cas的应用
1. AtomicInteger
2. 自旋锁
3. 令牌桶限流器

### 自旋锁
参考同上cas的链接
概念：线程尝试获取锁失败后，如果非自旋锁的情况下CPU会切换状态，使当前线程休眠，切换其他线程执行。而自旋锁不放弃CPU时间片，通过自旋等待锁释放。自旋锁的实现原理同样也是CAS。
为什么有自旋锁：阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。
缺点:不能代替阻塞，自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当挂起线程。

### 自适应自旋锁
自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。

### 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁
参考同上cas的链接
![](20210610_1_suo)
这四种锁是指锁的状态，专门针对synchronized的。synchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的。
synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”。
目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。

### 无锁
无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。

###  偏向锁
偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。
当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。
偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。

目的是：为了在没有多线程竞争的情况下尽量减少不必要的轻量级锁执行路径。因为轻量级锁的加锁解锁操作是需要依赖多次CAS原子指令的，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗也必须小于节省下来的CAS原子指令的性能消耗）。

### 轻量级锁
是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。
对象Mark Word的锁标志位设置为“00”
若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。

轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。

### 重量级锁
升级为重量级锁时，锁标志的状态值变为“10”。此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。

综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。

### 公平锁 VS 非公平锁
公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。
优点：等待锁的线程不会饿死。
缺点：整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。

非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。
优点：可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。
缺点：处于等待队列中的线程可能会饿死，或者等很久才会获得锁。

###  可重入锁 VS 非可重入锁
可重入锁
概念：是一个线程获取到一个对象的锁，未释放锁的情况下可以再次获取该对象的锁来执行其他加锁方法。
优点：避免死锁

示例代码：
```
public class Widget{
    public synchronized void do1(){
        System.out.println("1");
        do2();
    }
    public synchronized void do2(){
        System.out.println("2");
    }
}

static Widget widget = new Widget();
widget.do1();
widget.do2();
```
当一个线程获取到widget对象的锁时,执行do1方法，因为synchronized是可重入的，所以能再执行do2方法。如果不可重入就出现死锁。

### 独享锁 VS 共享锁
独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。
共享锁是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。

### java8对synchronized的优化
java6引进四种锁的状态，无锁，偏向锁，轻量级锁，重量级锁，（偏向锁和轻量级锁应该用的cas原理）
java8对cas进行优化，同时大量线程cas，会导致都不能成功，使用分段cas的思路，内部维护一个cell数组，分别对数组的一个位置操作，最后将结果累加。比如cell大小为10，100个线程，则10个线程对cell[0]自增，10个线程对cell[1]自增。。。

### JUC包有哪些类
java.util.concurrent 即java的并发包
1. ReentrantLock
2. AtomicInteger
3. CountDownLatch
4. ConcurrentHashmap
5. ThreadPoolExecutor
6. .....

### aqs解释
参考<https://zhuanlan.zhihu.com/p/86072774>
AQS就是一个并发包的基础组件，用来实现各种锁，各种同步组件的。
它包含了state变量、加锁线程、等待队列等并发中的核心组件。
ReentrantLock底层是基于AQS来实现的。
AbstractQueuedSynchronizer，抽象队列同步器
![](20210611_1_aqs)
![](20210611_2_aqs)
加锁时，线程1用cas方式将state变量置为1，成功则是加锁成功，加锁线程记录当前获取锁的是线程1。线程1使用重入锁会将state变量值再加1，线程2获取锁会先cas尝试将state由0变为1，失败则判断当前获取锁的线程是不是自己，不是则线程2加入等待队列。

### 线程同步的方法
1. synchronized
2. reentrantlock
3. linkedBlockingQueue
4. AtomicInteger

## mysql

### 数据库事务四大特性？
ACID
6.1.1 原子性（Atomicity）
原子性是指事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败。
6.1.2 一致性（Consistency）
事务必须使数据库从一个一致性状态变换到另外一个一致性状态。
> 举例说明：张三向李四转100元，转账前和转账后的数据是正确的状态，这就叫一致性，如果出现张三转出100元，李四账号没有增加100元这就出现了数据错误，就没有达到一致性。

6.1.3 隔离性（Isolation）
事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。
6.1.4 持久性（Durability）
持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。

### 数据库事务隔离级别（泛指所有数据库，不是特指mysql）
1. 未提交读（RU）（会出现脏读）
2. 提交读（RC）  （不可重复读）
3. 可重复读（RR）（Innodb默认）（会出现幻读）
4. 串行化 
详细介绍：<https://github.com/Snailclimb/JavaGuide/blob/main/docs/database/mysql/transaction-isolation-level.md>

### Mysql MVCC
mysql中提交读，可重复读 是通过MVCC（多版本并发控制）来实现的。
mvcc只在提交读和可重复读的隔离级别下才起作用。
mvcc原理分析：<https://juejin.cn/post/6844904194206351373>
> 但是这个博客在讲 REPEATABLE READ 下第一张图里 min_trx_id 值写错了，应该是101。

### mysql事务执行过程

### 为什么项目中mysql隔离级别选择提交读
1. RR会产生间隙锁，增加死锁概率，影响效率
2. RR条件列未命中索引，会锁表，RC只锁行
3. RC可以半一致性读，update语句在加锁的行上不会等待锁
参考<https://zhuanlan.zhihu.com/p/59061106>

### 共享锁，排它锁
共享锁和排它锁 和数据库隔离级别无关，任何隔离级别都可以有这两个锁

共享锁又称为读锁，简称S锁。多个事务的查询语句可以共用一把共享锁；
例如事务1在id=1的记录加上共享锁，事务2可以读id=1的记录，但不能修改删除
使用共享锁
```
SELECT * FROM `test` WHERE `id` = 1 LOCK IN SHARE MODE;
```
多个事务的查询语句可以共用一把共享锁；
如果只有一个事务拿到了共享锁，则该事务可以对数据进行 UPDATE DELETE 等操作；
如果有多个事务拿到了共享锁，则所有事务都不能对数据进行 UPDATE DELETE 等操作。

排他锁又称为写锁，简称X锁，顾名思义，排它锁不能与其它锁并存，而且只有一个事务能拿到某一数据行的排它锁，其余事务不能再获取该数据行的所有锁。
使用排它锁
```
SELECT * FROM `test` WHERE `id` = 1 FOR UPDATE;
```

### 什么是间隙锁
间隙锁说明：
当我们用范围条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。
例如
```
现有数据 
id age
1  10
2  20
3  40
4  50

SELECT * FROM child WHERE age >= 20 and age<=30 FOR UPDATE;
```
间隙锁会将age20-40范围内数据锁住（间隙锁会产生在现有数据之间，而不是20-30的间隙），其他事物不能在这个区间插入或修改数据

间隙锁的启用：mysql隔离级别设置为RR，并且关闭参数innodb_locks_unsafe_for_binlog （show variables like 'innodb_locks_unsafe_for_binlog'; 查看是否启用间隙锁）

MySQL InnoDB支持三种行锁定方式：
行锁（Record Lock）：锁直接加在索引记录上面。
间隙锁（Gap Lock）：锁加在不存在的空闲空间，可以是两个索引记录之间，也可能是第一个索引记录之前或最后一个索引之后的空间。
临键锁（Next-Key Lock）：行锁与间隙锁组合起来用就叫做Next-Key Lock。

间隙锁只存在于RR和串行话隔离级别中，RC和RU隔离级别只会产生行锁。

where条件列是唯一索引时，只有锁住多条记录或者一条不存在的记录的时候，才会产生间隙锁，指定给某条存在的记录加锁的时候，只会加记录锁，不会产生间隙锁。
where条件列是普通索引时，不论条件是单条还是范围，都会产生间隙锁。
where条件列没有索引时，不论条件是单条还是范围，都会产生间隙锁。和普通索引的区别在于会先对全表加锁，然后再释放不满足条件的记录，对满足条件的范围记录加锁，所以在没有索引时，不满足条件的数据行会有加锁又放锁的耗时过程。

参考：<https://www.cnblogs.com/rjzheng/p/9950951.html>

### mysql RR隔离级别下会产生幻读吗
结论：因为MVCC机制，可以防止读情况下的幻读，但是不能防止其他情况的幻读（例如update，delete）。但是一开始如果加上间隙锁，则可以防止所有情况的幻读。

详细说明：
|  id   | name  |
|  ----  | ----  |
| 1  | 后勤部 |

|  事务1                                                      | 事务2  |
|  ----                                                      | ----  |
| begin                                                      | begin |
| select * from dept                                         |       |
|                                                            |  insert into dept(name) values("研发部")  |
|                                                             |  commit |
| select * from dept (查出来一条记录，没有出现幻读，因为mvcc机制，新插入的数据版本高，不可见)  |    |
| update dept set name="财务部"   （会出现幻读，即更新了两条数据）    |   |
| commit                                                        |   |

结果
|  id   | name  |
|  ----  | ----  |
| 1  | 财务部 |
| 2  | 财务部 |

结论：第二次select没有出现幻读，update出现了幻读的情况，MySQL可重复读的隔离级别中mvcc机制并不是完全解决了幻读的问题，而是解决了读数据情况下的幻读问题。update会在最新的记录上操作。
但是如果第一次select 后面加上 for update 间隙锁，那么就不会出现幻读的情况


### 解决幻读的问题
解决方式是在select读时候的sql中增加for update  , 会把我所查到的数据锁住 , 别的事务根本插不进去 , 这样就解决了,这里用到的是mysql的next-key locks

SELECT ... FOR UPDATE 走的是IX锁(意向排它锁)，即在符合条件的rows上都加了排它锁，其他session也就无法在这些记录上添加任何的S锁或X锁。如果不存在一致性非锁定读的话，那么其他session是无法读取和修改这些记录的，但是innodb有非锁定读(快照读并不需要加锁)，for update之后并不会阻塞其他session的快照读取操作，除了select ...lock in share mode和select ... for update这种显示加锁的查询操作。

### select加锁分析
<https://www.cnblogs.com/rjzheng/p/9950951.html>

### Redo log, bin log, Undo log
InnoDB中通过undo log实现了数据的多版本，而并发控制通过锁来实现。  
undo log除了实现MVCC外，还用于事务的回滚。MySQL Innodb中存在多种日志，除了错误日志、查询日志外，还有很多和数据持久性、一致性有关的日志。  
binlog，是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的, 另外通过解析binlog能够实现mysql到其他数据源（如ElasticSearch)的数据复制。  
redo log 作用：确保事务的持久性。 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql 服务的时候，根据redo log 进行重做，从而达到事务的持久性这一特性。 内容：物理格式的日志，记录的是物理数据页面的修改的信息。
          记录了数据操作在物理层面的修改，mysql中使用了大量缓存，缓存存在于内存中，修改操作时会直接修改内存，而不是立刻修改磁盘，当内存和磁盘的数据不一致时，称内存中的数据为脏页(dirty page)。为了保证数据的安全性，事务进行中时会不断的产生redo log，在事务提交时进行一次flush操作，保存到磁盘中, redo log是按照顺序写入的，磁盘的顺序读写的速度远大于随机读写。当数据库或主机失效重启时，会根据redo log进行数据的恢复，如果redo log中有事务提交，则进行事务提交修改数据。这样实现了事务的原子性、一致性和持久性。
undo log: 除了记录redo log外，当进行数据修改时还会记录undo log，undo log用于数据的撤回操作，它记录了修改的反向操作，比如，插入对应删除，修改对应修改为原来的数据，通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本的数据，实现MVCC。  


binlog:默认1G，默认30天过期。Binlog日志是一种二进制日志，它记录了数据库的所有变更操作，包括数据的插入、更新和删除等操作。Binlog日志可以用于数据恢复、数据库复制和高可用性等场景。
相比于文本日志，二进制日志具有更高的效率和更好的安全性。二进制日志可以直接被数据库引擎读取和写入，避免了繁琐的文本解析过程。此外，二进制日志可以使用更复杂的算法来实现数据压缩和加密，保证了数据的安全性。
查看binlog内容：mysqlbinlog --start-datetime="2013-03-01 00:00:00" --stop-datetime="2014-03-21 23:59:59" /usr/local/mysql/var/mysql-bin.000007 -r  test2.sql
里面主要就是一些执行过的sql。
参考<https://www.jianshu.com/p/3eb4c44307c1>


### 读写分离？
master-slave模式，master用于写数据，slave用于读数据
### 主从同步
slave的io线程从master读取二进制日志binlog，（当master更新时，master线程被激活，并将二进制日志推送给slave，slave io线程读取网络上的二进制日志binlog）
并在本地保存为中继日志relaylog，然后sql线程读取中继日志relaylog的内容并执行命令，从而保证slave和master数据同步。

### 索引原理
Mysql myisam b+树 非聚簇索引
Mysql innodb b+树 聚簇索引

b树 b+树 hash索引
hash索引的缺点：Hash 索引不支持顺序和范围查询 例如
```
SELECT * FROM tb1 WHERE id < 500
```
b+树直接遍历比500小的叶子节点即可，hash索引要把id 1-499 的都要hash计算位置，太费时

b树的缺点：
- B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。（因此b+树一页可以存放更多的索引，树高度就会降低）
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。（因此b+适用于范围查询，效率高）

参考<https://github.com/Snailclimb/JavaGuide/blob/main/docs/database/mysql/mysql-index.md>

### 分库分表
见<https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/DB/mysql%E5%88%86%E5%8C%BA%E5%88%86%E8%A1%A8%E5%88%86%E5%BA%93%E6%96%B9%E6%A1%88.md>


### myisam为什么比innodb更适合读频繁，很少写的场景
1. innodb需要维护mvcc
2. innodb寻址要映射到块，再到行，MYISAM记录的直接是文件的OFFSET，定位比INNODB要快

### mysql选择哪一列建索引的依据
1. 列的数据发散
2. 列数据不变（是不是一点不能变）
3. 数据长度不能太长
4. 一个表不能建太多的索引
5. 联合查询的可以建联合索引

### sql的执行流程
1. mysql连接器做权限认证，用户名密码是否正确，该用户是否有该select语句的权限
2. 查询缓存
3. 分析器，词法分析和语法分析，词法分析就是提取关键字，如select，表名，字段名，语法分析就是sql语法是否正确
4. 优化器，以它认为的最优的执行方案去执行，比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。
5. 执行器，执行之前再校验一遍权限，调用innodb引擎的接口来执行sql

6. 如果是update的话，写undo log,用于提交失败后回滚
7. 去B+树种根据索引找到这一行数据，如果索引不是主键索引，并且查询的是不止索引字段，还要再回表一次，去主键索引的B+树查找，找到需要的那一行数据，返回
8. 判断数据页是否在内存中，是否是唯一性索引，来将update操作更改内存中的数据页或者更改磁盘文件，或者将操作缓存到changebuffer
9. 写redolog 将redo log设置为prepare状态。
10. 写binlog,redo log改成commit状态，提交事务

### mysql慢sql问题排查
1.偶尔很慢
   1.1 数据库在刷新脏页，update更新数据是先写redo log日志，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。如果redolog日志满了，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，就会导致我们平时正常的SQL语句突然执行的很慢
   1.2 拿不到锁，如果要判断是否真的在等待锁，我们可以用 show processlist这个命令来查看当前的状态
   
2. 一直很慢
    2.1 where字段没有索引
    2.2 没用上索引，例如where c-1=100,where pow(c,2) = 1000,
    2.3 数据库选错索引，例如，select * from t where 100 < c and c < 100000，走索引的话需要走两次，一次c索引，一次主键索引，可能不如扫描，只要扫描一次。所以mysql怎么判断走不走索引，采样部分数据，如果数据发散厉害，范围大，就走索引，如果发散范围不大，就不走索引，但是采样可能出现错误，刚好采样到范围小的数据，就导致选错索引。解决办法：强制使用索引，select * from t force index(a) where c < 100 and c < 100000;  采样错误也会导致where多个字段，有索引的话，选错索引
    
登录到MySQL中进行查看是否有事物未提交，是否有发生锁等待。
select * from information_schema.INNODB_TRX
select * from information_schema.PROCESSLIST #查看当前的SQL执行情况。主要观察是否有 Waiting for table metadata lock 或者表锁、全局读锁等SQL。

看看是否有大量临时表

### undo log是什么？
undo log主要是保证事务的原子性，事务执行失败就回滚，用于在事务执行失败后，对数据回滚。undo log是逻辑日志，记录的是SQL。（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。）

### change buffer是什么
（就是将更新数据页的操作缓存下来）
在更新数据时，如果数据行所在的数据页在内存中，直接更新内存中的数据页。
如果不在内存中，为了减少磁盘IO的次数，innodb会将这些更新操作缓存在change buffer中，在下一次查询时需要访问这个数据页时，在执行change buffer中的操作对数据页进行更新。
适合写多读少的场景，因为这样即便立即写了，也不太可能会被访问到，延迟更新可以减少磁盘I/O，只有普通索引会用到，因为唯一性索引，在更新时就需要判断唯一性，所以没有必要。

### redo log 是什么？
redo log就是为了保证事务的持久性。因为change buffer是存在内存中的，万一机器重启，change buffer中的更改没有来得及更新到磁盘，就需要根据redo log来找回这些更新。
优点是减少磁盘I/O次数，即便发生故障也可以根据redo log来将数据恢复到最新状态。
缺点是会造成内存脏页，后台线程会自动对脏页刷盘，或者是淘汰数据页时刷盘，此时收到的查询请求需要等待，影响查询。

### InnoDB事务实现原理
acid
原子性：
靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。
undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。
以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。

一致性：

隔离性:
(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

持久性：
靠的是redo log。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。

### innodb和myisam的区别
1. innodb不保存表的具体行数
为什么innodb不保存：
因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询
2. InnoDB 支持事务，MyISAM 不支持事务
3. InnoDB 支持外键，而 MyISAM 不支持
4. InnoDB 是聚集索引，MyISAM 是非聚集索引。
5. InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁


### 缓解数据库压力的办法
1. 缓存
2. 分库分表
3. 读写分离
4. 合理增加索引


### 聚簇索引和非聚簇索引区别
1. 聚簇索引的主键索引叶节点存放整行数据，非聚簇索引叶节点存放数据地址
2. 聚簇索引的数据的物理存放顺序与索引顺序是一致的
3. 聚簇索引的辅助索引是指向主键索引，而非聚簇索引的辅助索引（也即二级索引）是指向数据的物理地址。即每个索引相对独立，查询用到索引时，索引指向数据的位置。

> innodb使用聚簇索引，myisam使用非聚簇索引


### 聚簇索引的好处
1. InnoDB在移动行时无须更新辅助索引中的这个"指针"
2. 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。
3. 聚簇索引适合用在排序的场合，非聚簇索引不适合
4. 取出一定范围数据的时候，使用用聚簇索引
5. 非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。

### 聚簇索引劣势
1. 二级索引访问需要两次索引查找
2. 插入速度严重依赖于插入顺序
3. 更新主键的代价很高，因为将会导致被更新的行移动
4. 采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多（https://juejin.im/post/6844903845554814983）

### 覆盖索引
即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能。
之前我们已经建立了表student，那么现在出现的业务需求中要求根据名称获取学生的年龄，并且该搜索场景非常频繁，
那么先在我们删除掉之前以字段name建立的普通索引，以name和age两个字段建立联合索引。
查询后，在name,age联合索引树上找到名称为小李的节点
此时节点索引里包含信息age 直接返回 12

### 索引下推
用户表(user)，其中主要几个字段有：id、name、age、address。建立联合索引（name，age）。
SELECT * from user where  name = '陈' and age=20， 这是用了组合索引
SELECT * from user where  name like '陈%' and age=20，因为name遇到非等值判断，组合索引停止，所以用不到age的索引，
mysql5.6以前的版本会将查到的4个主键id，回表，去主键索引查询，要查4次
mysql5.6及之后优化，有索引下推，即在二级索引里，判断age是否等于20，判断完找到唯一的主键id，再回表，去主键索引查询，只要查1次

参考<https://zhuanlan.zhihu.com/p/121084592> 以及第一条评论

### sql：while col1 like '%key' 怎么用索引
// 创建一个函数索引
CREATE INDEX inde_1 ON table1(REVERSE(col1));
// 使用
SELECT * FROM table1 WHERE REVERSE(col1) LIKE REVERSE('%ABC');



### explain除了看索引还看什么
mysql> explain select * from staff;
+----+-------------+-------+------+---------------+------+---------+------+------+-------+
| id | select_type | table | type | possible_keys | key  | key_len | ref  | rows | Extra |
+----+-------------+-------+------+---------------+------+---------+------+------+-------+
|  1 | SIMPLE      | user  | ref  | name_ix       | name_ix | 514  | const|    1 | NULL  |
+----+-------------+-------+------+---------------+------+---------+------+------+-------+
1 row in set

type：这是最重要的字段之一，显示查询使用了何种类型。从最好到最差的连接类型依次为：
system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL
一般来说，得保证查询至少达到range级别，最好能达到ref。

key:
列显示MySQL实际决定使用的键（索引）

rows: 也是一个重要的字段。 这是mysql估算的需要扫描的行数（不是精确值）。

extra: 常见的有以下几种内容:
  distinct：在select部分使用了distinct关键字
  Using filesort：当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大.
  Using index:"覆盖索引扫描", 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错
  Using temporary:查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.


参考<https://www.jianshu.com/p/8fab76bbf448>
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_3.png)


### mysql分库分表 平滑扩容方案

### 数据库平滑扩容的方案
<https://zhuanlan.zhihu.com/p/37792971>
<https://cloud.tencent.com/developer/article/1420754>

### mysql索引结构分类
B数，R树，hash，全文索引
R树适合范围查询，仅支持geometry数据类型
full-text（全文索引）在mysql里仅有myisam支持它，而且支持full-text的字段只有char、varchar、text数据类型。
full-text主要是用来代替like "%***%"效率低下的问题

### mysql主从  // todo
mysql主从的作用：
1、数据热备：作为后备数据库，主数据库服务器故障后，可切换到从数据库继续工作，避免数据丢失。
2、架构的扩展：业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。
3、读写分离使数据库能支撑更大的并发。在报表中尤其重要。由于部分报表sql语句非常的慢，导致锁表，影响前台服务。如果前台使用master，报表使用slave，那么报表sql将不会造成前台锁，保证了前台速度。

主从原理：
从库会生成两个线程,一个 I/O 线程,一个 SQL 线程；
I/O 线程会去请求主库的 binlog，并将得到的 binlog 写到本地的 relay-log（中继日志）文件中；主库会生成一个 log dump 线程,用来给从库 I/O 线程传 binlog；
SQL 线程，会读取 relay log 文件中的日志，并解析成sql语句逐一执行。
参考<https://dongzl.github.io/2020/03/15/12-MySQL-Master-Slave-Replication/index.html>

主从一致：

主从延迟：
原因：
主库写入量过大
网络延迟
从库的压力大（大量查询放在从库上，结果导致从库上耗费了大量的CPU资源，进而影响了同步速度，造成主从延迟。）
大事务的执行。

解决：
配合 semi-sync 半同步复制（主库只需要等待至少一个从库接收到并写到 Relay Log 文件即可，主库不需要等待所有从库给主库返回 ACK。主库收到这个 ACK 以后，才能给客户端返回 “事务完成” 的确认。）
一主多从，分摊从库压力；
降低多线程大事务并发的概率，优化业务逻辑
优化SQL，避免慢SQL，减少批量操作
实时性要求的业务读强制走主库

参考<https://www.cnblogs.com/hefeng2014/p/17448298.html>
<https://developer.aliyun.com/article/894940>
<https://cloud.tencent.com/developer/article/2240328>




## 集中排序算法复习下

## elasticsearch
### Elasticsearch是什么？
- elasticsearch 是一个分布式的实时搜索和分析引擎

- Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。
- es⽂档写操作是分⽚上⽽不是节点上，先写在主分⽚，主分⽚再同步给副分⽚，因为主分⽚可以分布在不同的节点上，所以当集群只有⼀个master节点的情况下，即使流量的增加它也不会成为瓶颈，就算它挂了，任何节点都有机会成为主节点。
### 为什么使用elasticsearch
1. 存储的是非文档型数据
2. Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快
3. 文本搜索
4. 相关度排名

### es数据类型
字符串：text、keyword
数值： long, integer, short, byte, double, float
布尔值：boolean
时间：date
数组

### es的架构原理 todo
分布式，分片，主分片，副分片，segment，

### 什么是倒排索引
即根据文章内容中的关键字建立索引，索引会指向包含这个关键字的文档

### 默认分词器
standard分词器

### es的query大量数据报错的问题
```
SearchQuery query = new NativeSearchQueryBuilder().withQuery(matchAllQuery())
                .withTypes(TYPE_INTELLIGENT_COLLECT_LABEL.getType())
                .withIndices(EsIndex.INDEX_INTELLIGENT_BASE.getName())
                .withPageable(new PageRequest(0, 200,
                        new Sort(Sort.Direction.DESC,"createAt"))).build();

        Iterable<EntityCollectLabel> iterable = elasticsearchTemplate.queryForList(query,EntityCollectLabel.class);
```
这个方法会报数据量过大的错误
```
Caused by: QueryPhaseExecutionException[Result window is too large, from + size must be less than or equal to: [500000] but was [510000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.]
  at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:212)
```
需要用游标的方式取数据sroll api
```
String scrollId = elasticsearchTemplate.scan(searchQuery,55000,false);
 Page<EntityTrackingDi> page = elasticsearchTemplate.scroll(
                    scrollId, 55000L , new SearchResultMapper() {
```
原因 ：
***ElasticSearch的默认深度翻页机制***
ES默认的分页机制一个不足的地方是，比如有5010条数据，当你仅想取第5000到5010条数据的时候，ES也会将前5000条数据加载到内存当中，所以ES为了避免用户的过大分页请求造成ES服务所在机器内存溢出，默认对深度分页的条数进行了限制，默认的最大条数是10000条，

> 其实mysql的limit 100000,100 也是将十万多条数据加载到内存，也有同样的问题，但是mysql可以通过sql语句优化 比如
Select * From table Where ID>=(
    Select ID From table order by ID limit 100000,1
) order by ID limit 100;
SELECT id,title,content FROM items WHERE id IN (SELECT id FROM items ORDER BY id limit 900000, 10);
先查询id，这样加载到内存的只有id数据，不是全部数据。

scroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索

es 提供了 scroll 的方式进行分页读取。原理上是对某次查询生成一个游标 scroll_id ， 后续的查询只需要根据这个游标去取数据，直到结果集中返回的 hits 字段为空，就表示遍历结束。scroll_id 的生成可以理解为建立了一个临时的历史快照，在此之后的增删改查等操作不会影响到这个快照的结果。

### es深度分页解决方案  scrollid会变吗
<https://my.oschina.net/u/1787735/blog/3024051>

### es调优，慢查询怎么处理

### es相比于mysql的区别
1. es是文档格式的存储，不需要定义字段。适合全文检索、日志查询
1. 业务宽表（数据库字段太多，查询太慢，索引没有办法再做优化）
2. 索引查询速度比mysql索引快   // todo 为什么
3. 天然分布式，容易横向扩容。支持大数据量，高性能。
4. es不是实时的，mysql是
5. 无法联表查询，不支持事务

### es查询、更新、删除过程。


## 中间件
## mq消息队列
### 优缺点
优点：解耦，异步，消峰
缺点：系统复杂性增加，系统的可用性降低

### 目前主流mq类型
RocketMQ 
rabbitmq
Kafka

### rabbitmq kafka rocketmq 区别
1. kafka：吞吐量高，消息失败不支持重试,轻量级的，高效的但是不保证安全的。一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务。
2. rabbitmq:低延迟，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。有消息确认机制，和持久化功能。
开源的，社区比较稳定的支持，活跃度也高，社区文档资料完善些
3. RocketMQ 阿里开源的消息中间件，它是纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。
// todo rabbitmq和RocketMQ的区别

选型：
所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。
如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。


|          |  rabbitmq        | rocketmq              |   kafka    |
|  ----    |  ----            | ----                  |   ---- |
| 吞吐量    |  中等（万级）       | 高（十万级）             | 极高（百万级）    |
| 延迟      |  低延迟（毫秒级）   | 中等                    | 延迟略高（默认需刷盘）    |
| 消息可靠性 |  高（ACK + 持久化） | 高（ACK + 异步/同步刷盘） | 高（通过副本 + ACK）    |
| 消息重试   |  支持（死信队列）     | 支持（延迟消息）         | 不内建（需业务控制或外部组件）   |
| 事务消息   |  不支持             | 支持（分布式事务）        | 支持（幂等 + 两阶段提交）  |
| 运维复杂度 |  简单                | 一般                      | 较复杂（Zookeeper依赖）  |
| 使用场景   |  通用异步解耦、低延迟场景| 分布式事务、电商订单、金融消息 | 大数据日志、埋点、实时流处理  |

### rabbitmq原理
消息发送消费流程：
1. producer发送消息到mq的exchange组件
2. exchange组件根据routing key将message分发到对应的queue1中
3. queue1将消息发送到consumer
4. consume发送ack确认消息到queue1
5. queue1收到ack，删除队列中的message
（如果queue1没收到ack，则会一直发送message，即消息丢失超时重发）
参考<https://zhuanlan.zhihu.com/p/281912931> 

### rabbitmq的exchange路由规则
1. Fanout    （全局转发）
会将消息发送到所有和它进行绑定的队列上。
2. Direct   （根据routing key精确匹配）
通过消息上的路由键直接对消息进行分发。
3. Topic  （模糊匹配）
会将routing key和binding key进行通配符匹配。

### rabbitmq消息丢失处理策略：
消息丢失分为三种，1.发送方消息发送失败，2.exchange丢失消息，3.接收方未接受到消息
处理策略：
1.发送方开启confirm模式，每个消息会带一个唯一id，exchange收到消息会返回带id的确认消息。
2.exchange持久化消息（可以持久化后再返回确认消息）
3.接收方接受消息后返回ack确认，自动模式（默认，一接收到就返回ack，可能没来得及处理，或者选择手动ack，处理完消息才返回ack）
exchange返回ack后，发送方会有个回调函数就行处理，知道是成功还是失败，失败的话重发消息（消息哪来的，可以存进内存，比如用hashmap），如果ack丢失，或者就是没接收到，发送方设置超时时间，超时则重发消息。

### 假如是ack在网络中丢失，发送方或者exchange重新发送消息，接收方重复消费怎么办？
在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重和幂等的依据（消息投递失败并重传），避免重复的消息进入队列；
在消息消费时，要求消息体中必须要有一个bizId（对于同一业务全局唯一，如支付ID、订单ID、帖子ID等）作为去重和幂等的依据，避免同一条消息被重复消费。

### mq消息的顺序性
1.通过某种算法，将需要保持先后顺序的消息放到同⼀个消息队列中(kafka中就是partition,rabbitMq中就是queue)。然后只⽤⼀个消费者去消费该队列。
2.可以在消息体内添加全局有序标识来实现。

### mq消息积压  // todo 
增加队列数和消费者的数量
限制生产这产生消息的速度
机器上创建临时的消息队列，把消息积压队列中的消息取出来，放到临时队列里面去。



## spring相关
### Spring中的设计模式
// todo 手写几种设计模式

1. 单例模式（bean ioc）
一般用这种就行了，如果要懒加载（即用到这个类变量再实例化，详见下面链接的第4和第5中实现方式）
```
// 浪费内存，还没用就初始化
public class Singleton {  
    private static Singleton instance = new Singleton();  
    private Singleton (){}  
    public static Singleton getInstance() {  
    return instance;  
    }  
}
// 最优解
public class Singleton {  
    private volatile static Singleton singleton;  
    private Singleton (){}  
    public static Singleton getSingleton() {  
      if (singleton == null) {  
          synchronized (Singleton.class) {  
              if (singleton == null) {  
                  singleton = new Singleton();  
              }  
          }  
      }  
    return singleton;  
    }  
}
```
参考<https://www.runoob.com/design-pattern/singleton-pattern.html>
2. 工厂模式（bean ioc）
实现参考<https://www.runoob.com/design-pattern/factory-pattern.html>
3. 代理模式（aop）
实现参考<https://www.runoob.com/design-pattern/proxy-pattern.html>
4. 观察者模式（applicationListener）
实现参考<https://www.runoob.com/design-pattern/observer-pattern.html>
   
// 为什么动态jdk代理模式要有接口
因为代理类要实现接口
// 为什么cglib代理不需要接口
这种是继承类来实现的
// 静态代理是什么
// 为什么要用单例模式，好处是什么

### spring 启动过程
1️⃣ 创建 SpringApplication 对象
      读取主类上的注解（如 @SpringBootApplication）
      推断项目类型（普通项目、Web 项目、Reactive 项目）
2️⃣ 初始化环境（Environment）
      加载系统环境变量、application.properties / yml 配置
      设置配置项（比如端口、开发模式等）
3️⃣ 创建并准备 Spring 容器（ApplicationContext）
      创建 AnnotationConfigServletWebServerApplicationContext（如果是 Web 项目）
      注册一些基本组件（比如 BeanFactory、Environment）
4️⃣ 加载 Bean 定义并初始化 Bean
      扫描所有带注解的类（如 @Component, @Service, @Repository）
      创建 BeanDefinition（描述 Bean 的元信息）
      调用 Bean 的构造方法，自动注入依赖
5️⃣ 调用 Runner 并完成启动
      调用 ApplicationRunner / CommandLineRunner
      应用启动完成，容器准备就绪

<https://search.bilibili.com/all?keyword=spring%E5%90%AF%E5%8A%A8&from_source=web_search>
<spring_start.png>

### spring事务
参考：<https://zhuanlan.zhihu.com/p/114461128>

### 事务作用
可以让一系列操作变成一个原子操作，要么一起成功，要么一起失败，用于转账等类似场景

### 编程式事务
```
try {
    //TODO something
     transactionManager.commit(status);
} catch (Exception e) {
    transactionManager.rollback(status);
    throw new InvoiceApplyException("异常失败");
}
```
参考<https://cloud.tencent.com/developer/article/1697221>

### 声明式事务
```
@Transactional(rollbackFor = Exception.class)
    @GetMapping("/test")
    public String test() {
        int insert = cityInfoDictMapper.insert(cityInfoDict);
    }
```

### @Transactional 原理
在bean进行创建出实例时， 如果是有事务注解的方法，就会被进行增强，最终形成代理类。
在spring中，有两种动态代理的方式，一种是jdk，它是将原始对象放入代理对象内部，通过调用内含的原始对象来实现原始的业务逻辑，
而另一种是cglib，它是通过生成原始对象的子类，子类复写父类的方法，从而实现对父类的增强。
参考<https://www.51cto.com/article/753669.html>

### 基于@Transactional注解
@Transactional 可以作用在接口、类、类方法。
- 作用于类：当把@Transactional 注解放在类上时，表示所有该类的public方法都配置相同的事务属性信息。
- 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖类的事务配置信息。
- 作用于接口：不推荐这种使用方法，因为一旦标注在Interface上并且配置了Spring AOP 使用CGLib动态代理，将会导致@Transactional注解失效
另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。

### 传播级别
默认值为 Propagation.REQUIRED

- @Transactional(propagation=Propagation.REQUIRED) ：如果当前存在事务，则加入该事务，如果当前不存在事务，则创建一个新的事务。( 也就是说如果A方法和B方法都添加了注解，在默认传播模式下，A方法内部调用B方法，会把两个方法的事务合并为一个事务 ）
- @Transactional(propagation=Propagation.NOT_SUPPORTED) ：以非事务的方式运行，如果当前存在事务，暂停当前的事务。
- @Transactional(propagation=Propagation.REQUIRES_NEW) ：不管是否存在事务,都创建一个新的事务,原来的挂起,新的执行完毕,继续执行老的事务
- @Transactional(propagation=Propagation.MANDATORY) ：如果当前存在事务，则加入该事务；如果当前不存在事务，则抛出异常。
- @Transactional(propagation=Propagation.NEVER) ：以非事务的方式运行，如果当前存在事务，则抛出异常。
- @Transactional(propagation=Propagation.SUPPORTS) ：如果当前存在事务，则加入该事务；如果当前不存在事务，则以非事务的方式继续运行。
- @Transactional(propagation=Propagation.NESTED) ：和 Propagation.REQUIRED 效果一样。

### 隔离级别
默认是使用数据库的隔离级别

@Transactional(isolation = Isolation.READ_UNCOMMITTED)：读取未提交数据(会出现脏读, 不可重复读) 基本不使用
@Transactional(isolation = Isolation.READ_COMMITTED)：读取已提交数据(会出现不可重复读和幻读)
@Transactional(isolation = Isolation.REPEATABLE_READ)：可重复读(会出现幻读)
@Transactional(isolation = Isolation.SERIALIZABLE)：串行化

### spring事务失效的情况
参考<https://zhuanlan.zhihu.com/p/114461128>
1. 方法自调用
a方法没@transactional，b方法有@transactional，a调用b，代理类调用a，这时候不会触发b方法的事务。
(其实这还是由于使用Spring AOP代理造成的，因为只有当事务方法被当前类以外的代码调用时，才会由Spring生成的代理对象来管理。)
解决方法：
 - a上加@transactional，
 - 类上加@transactional
 - a中这样调用b ((A)AopContext.currentProxy).b()
 - 把b方法抽到另一个类中
2. 方法不是public
因为cglib或jdk代理的方法会调用computeTransactionAttribute方法，这个方法会检查目标方法是否是public，不是 public则不会获取@Transactional 的属性配置信息。protected、private 修饰的方法上使用 @Transactional 注解，虽然事务无效，但不会有任何报错
3. rollbackFor 设置错误，默认是RuntimeException，其他异常不会回滚
4. myisam引擎不支持事务
5. 事务方法里使用了try-catch
6. propagation设置错误 supports not_supported never

### spring的事务隔离级别和mysql的事务隔离级别不一致会这么样？
以 Spring 事务为准


### spring如何解决循环依赖
### 产生循环依赖的代码
属性注入：
```
@Service
public class CircularServiceA {
    @Autowired
    private CircularServiceB circularServiceB;
}
@Service
public class CircularServiceB {
    @Autowired
    private CircularServiceC circularServiceC;
}
@Service
public class CircularServiceC {

    @Autowired
    private CircularServiceA circularServiceA;

}
```


### 解决循环依赖
首先，需要明确的是spring对循环依赖的处理有三种情况：
①构造器的循环依赖：这种依赖spring是处理不了的，直接抛出BeanCurrentlylnCreationException异常。
②单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。（上面的例子就是单例模式下的基于setter注入的循环依赖）
③非单例循环依赖：无法处理。

2的情况原理：
spring三级缓存
我们初始化一个Bean时，先调用Bean的构造方法，这个对象就在内存中存在了（对象里面的依赖还没有被注入），然后把这个对象保存下来，当循环依赖产生时，直接拿到之前保存的对象，于是循环依赖就被终止了，依赖注入也就顺利完成了。

https://blog.csdn.net/lkforce/article/details/97183065
https://juejin.im/post/6844903806757502984

> setter注入：
  ```
  @Controller
  public class FooController {
    private FooService fooService;
    @Autowired
    public void setFooService(FooService fooService) {
        this.fooService = fooService;
    }
  }
  ```
构造器注入：
  ```
  @Controller
  public class FooController {
    
    private final FooService fooService;
    
    @Autowired
    public FooController(FooService fooService) {
        this.fooService = fooService;
    }
  }
  ```

### java怎么解决循环依赖
1. 重新设计类结构
2. 注解 @Lazy
```
@Component
public class CircularDependencyA {
 
    private CircularDependencyB circB;
 
    @Autowired
    public CircularDependencyA(@Lazy CircularDependencyB circB) {
        this.circB = circB;
    }
}
```
3. 使用Setter/Field注入
```
@Autowired
public void setCircB(CircularDependencyB circB) {
    this.circB = circB;
}
```
4. 使用@PostConstruct
```
@Component
public class CircularDependencyA {
 
    @Autowired
    private CircularDependencyB circB;
 
    @PostConstruct
    public void init() {
    }
        circB.setCircA(this);
 
    public CircularDependencyB getCircB() {
        return circB;
    }
}
```
5. 实现ApplicationContextAware与InitializingBean

参考<https://blog.csdn.net/Revivedsun/article/details/84642316>



### @autowired原理
根据类型自动装配
@Autowired   
@Qualifier("userServiceImpl")   
public IUserService userService;
加上@Qualifier 可以根据名字就行装配


### spring自动注入
@Autowired 根据类型注入，
@Resource 默认根据名字注入，其次按照类型搜索
@Autowired @Qualifie("userService") 两个结合起来可以根据名字和类型注入

### spring bean初始化流程

### bean生命周期
参考<https://www.bilibili.com/video/BV1584y1r7n6/?spm_id_from=333.788&vd_source=c0230cf1a761ed5500a78a5622b84f70>
一 生产bean
1. 准备容器环境
2. 加载bean定义（找到bean定义的类，统一放在beanDefinitionMap中）
3. 创建bean对象
   3.1 构造对象。用反射机制，从bean定义中拿到构造方法，然后找构造方法的参数，从单例池中根据class查找，如果有多个实例，再根据参数名匹配。然后构造对象，即实例化
   3.2 填充属性。通过三级缓存机制进行填充
   3.3 初始化实例。通过initializeBean方法初始化
   3.4 注册销毁。注册实现销毁接口的bean，这样在销毁时可以执行destory方法了
4. 将创建好的bean对象通过addSingleton方法放在单例池中
   二 使用bean
    5. 使用bean
       三 销毁bean
6. 执行销毁前处理器，会执行bean中@PreDestroy方法。
7. 然后通过destroyBeans方法逐一销毁bean，销毁时会执行destroy方法
8. 执行bean上自定义的destroyMethod方法

### bean加载过程
解析配置文件：通过读取配置文件（如 XML 或注解），Spring 可以了解到 Bean 的相关信息，如类名、作用域、依赖等。
实例化 Bean：根据配置文件中的信息，Spring 会使用反射来实例化 Bean。
初始化：
属性赋值 ，bean有很多依赖的对象，那么spring会依次调用这些依赖的对象进行实例化
初始化好的bean放入到spring的缓存中、填充我们预设的属性进一步做后置处理等
Bean 的销毁：当 Spring 应用程序结束时，Spring 会调用 Bean 的 destroy 方法，以便进行清理和销毁。


### springboot启动过程
参考视频<https://www.bilibili.com/video/BV12Y411L7dv/?spm_id_from=333.337.search-card.all.click&vd_source=c0230cf1a761ed5500a78a5622b84f70>
<https://www.bilibili.com/video/BV1e14y1A7pT/?spm_id_from=333.788&vd_source=c0230cf1a761ed5500a78a5622b84f70>
主要分为两大部分
1. 创建SpringApplication实例化对象
   1.1 获取应用类型，默认servlet
   1.2 读取spring.factories文件
   1.3 读取ApplicationContextInitializer的key的值，进行实例化（反射的方式）（获取初始化器的实例对象）
   1.4 读取ApplicationListener的key的值，进行实例化（获取监听器的实例对象）
   1.5 定位main方法（找到当前程序的主类）

2. 调用run方法
   2.1 启动监听器
   2.2 构造容器环境，加载配置信息
   2.3 创建容器（ApplicationContext，也叫上下文），创建容器过程中会构造，beanFactory，用来解析@Component等注解的配置类后处理器 ConfigurationClassPostProcessor，
   用来解析@Autowired，@Value等注解的自动注解bean后处理器AutowiredAnnotationBeanPostProcessor。
   2.4 容器初始化（加载启动类）
   2.5 刷新容器（创建BeanFactory对象，xml解析，给beanFactory设置一些属性值，国际化的支持，初始化bean）

> 启动类上的@SpringBootApplication注解是由3个注解组合而成（@EnableAutoConfiguration,@SpringBootConfiguration,@ComponentScan ）
@EnableAutoConfiguration是最为核心的，他会导入AutoConfigurationImportSelector类，这个类会将所有配置类都加载。
@SpringBootConfiguration等价与@Configuration，将这个类标记为配置类，被加载到容器中。
@ComponentScan自动扫描并加载符合条件的bealei
 
参考<https://juejin.cn/post/7144912826543865893>


### springcloud   // todo
#### Spring Cloud 是什么:
Spring Cloud是一系列框架的有序集合。简化了分布式系统基础设施的开发，如服务发现注册、配置中心、智能路由、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。

优点：配置比较简单、耦合度比较低、减轻团队的成本可以并行开发，微服务跨平台的
缺点：针对数据的管理比麻烦，系统集成测试比较麻烦，性能的监控比较麻烦

#### SpringCloud由什么组成：
Spring Cloud Eureka：服务注册与发现
Spring Cloud Zuul：服务网关
Spring Cloud Ribbon：客户端负载均衡
Spring Cloud Feign：声明性的Web服务客户端
Spring Cloud Hystrix：断路器
Spring Cloud Config：分布式统一配置管理


## 其他知识

### 接口高并发保护系统的方案
1.缓存（redis）    （例如将高频同时不需要实时更新的数据放在redis中）
2.限流            （单位时间内超出阈值的请求直接返回请等待或失败）
3.降级            （高峰期将非关键服务关闭，例如双11零点将其他的例如修改购物车地址，退货等服务关闭。用于解决资源不足的情况）

### 限流
1.信号量计数器方法
```
private final Semaphore permit = new Semaphore(10, true);
@PostMapping("/test")
    public String test(){
        try {
            permit.acquire();       // 如果已经有10个任务在处理，第11个请求会阻塞在这里
            log.info("处理请求===============>");
            Thread.sleep(2000);
        }catch (Exception e){
            log.error("error");
        }finally {
            permit.release();
        }
        return "success";
    }


```
2. 令牌桶算法
我们以一个恒定的速率向一个桶内放令牌，每次请求来的时候去桶里拿令牌，如果拿到了就继续后面的操作，如果没有拿到则等待。
```
public static void main(String[] args) {
        String start = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个
        for (int i = 1; i <= 10; i++) {
            double waitTime = limiter.acquire(i);// 请求RateLimiter, 超过permits会被阻塞
            System.out.println("cutTime=" + System.currentTimeMillis() + " call execute:" + i + " waitTime:" + waitTime);
        }
        String end = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        System.out.println("start time:" + start);
        System.out.println("end time:" + end);
    }
```
RateLimiter limiter = RateLimiter.create(1.0) 创建一个限流器，每秒生成1个令牌；

limiter.acquire(i) 以阻塞的方式获取令牌，随着i的增加，需要的令牌数增多，则需要等待的时间也增加。

相比计数方法的优点：可以方便的改变速度

> **如果是秒杀，库存问题**
则用redis存库存，redis库存减到0，不再处理请求


### 助力时网络故障，同一个人对同一个团助力两次
线程并发问题，两个线程同时查询助力表，发现用户a未对团b助力过，然后开始助力，这就等于a给b助力了两次，规则是一个人只能助力一次
解决办法：
1.对（a.id b.id）加唯一性索引
2.事务里代码逻辑：查询-插入-查询-校正，两个线程并发，其中一个第二次查询发现insert了两次就校正，如果两个线程都校正了，则返回助力失败。
3.没有设置唯一索引的话用 INSERT INTO table(field1, field2, fieldn) SELECT 'field1', 'field2', 'fieldn' FROM DUAL WHERE NOT EXISTS(SELECT field FROM table WHERE field = ?)
4.redis加分布式锁，以a的id和b的id为key加锁，锁的粒度为那一行数据。

### 更新开团表商品剩余价值时，并发执行问题
1. 可以用redis加分布式锁
2. 可以cas原理，比较version是否一样
3. 可以select  。。。。for update 加行锁


### 接口反应时间长的问题排查（接口慢）（接口变慢）
个别接口慢:
打日志、链路追踪工具（推荐）SkyWalking，Arthas ，确定慢的位置，是代码逻辑问题，死循环，还是sql执行太慢
一般都是sql慢，把sql日志打出来，或者explain，sql慢一般都是没有用到正确索引，或者数据量太大。
如果只是短暂慢，也有可能是mysql正在刷缓存。如果是调用第三方接口慢，那就找对应的负责人了

所有接口响应慢:
服务器问题，需要排查网络、CPU使用率、内存使用率、磁盘使用率等

1. 资源瓶颈 cpu是否100% （加机器）
2. 线程池配置的不合理（线程数配置的太少导致的请求积压）
3. 是否多表关联  把join逻辑放到业务代码里解决
2. 缓存没加 （加缓存）
3. 查看gc是否频繁GC，GC时间过长
4. 依赖第三方接口 （是不是第三方接口反应慢，能不能异步调用）
5. sql查询慢 （大量临时表，索引有没有加，explain有没有用到索引，是否需要强制需要用索引，数据量太大，要扩容）



### cpu100%排查 ? 待完善
1. 定位高负载进程 pid，使用top命令
2. 定位具体的异常业务，使用 pwdx 命令根据 pid 找到业务进程路径，进而定位到负责人和项目
3. 定位异常线程及具体代码行
   传统的方案一般是4步：

top oder by with P：1040 // 首先按进程负载排序找到  maxLoad(pid)
top -Hp 进程PID：1073    // 找到相关负载 线程PID
printf “0x%x\n”线程PID： 0x431  // 将线程PID转换为 16进制，为后面查找 jstack 日志做准备
jstack  进程PID | vim +/十六进制线程PID -        // 例如：jstack 1040|vim +/0x431 -


1. 定位高负载进程 pid，使用top命令
2. 显示线程列表 ps -mp pid -o THREAD,tid,time

### 订单超时自动取消实现方案
1. 定时轮询（quartz实现） 
   优点：实现简单，通过quartz框架进行任务调度，无其他依赖，支持集群部署。 
   缺点：简单粗暴的全表扫描方式对数据库性能影响特别大，可能影响其他正常的业务操作响应时效，另外配置扫描时间间隔也是个问题，
   
2. JDK延迟队列DelayQueue 
   优点：不需要任何第三方依赖，实现非常简单
   缺点：数据全部保存在JVM内存中，占用内存，可能会引发内存溢出，另外宕机或重启数据会全部丢失，无法做集群。

3. Redis key过期监听
   如果key失效后，redis会给客户端发送消息即pub/sub机制，从而实现延迟方案。
   优点：实现简单，redis内存操作，速度快，性能高，集群扩展方便，可存储大量订单数据，持久化机制使得故障时通过AOF或RDB方式恢复，适合对延迟精度要求不高的业务场景
   缺点：redis的key过期有惰性清除和定时清除两种策略，可能会存在延迟时间不精确的问题，另外redis的pub/sub 机制是不可靠的，如果客户端故障或重启期间有key过期则过期通知事件的数据就丢失了，从而订单无法过期，可以通过补偿机制配合使用，定时任务去做轮询补偿。

正确选择：
4. RabbitMQ消息队列
   消息发送后，有一定延时才会投递。
   优点：RabbitMQ消息服务可靠性高，消息处理速度快，支持大数据量，并且支持分布式横向扩展方便。
   缺点：引入RabbitMQ中间件系统复杂度增高，运维成本增加，使用起来配置较复杂。
   
### kingbase和mysql区别
kingbase基于PostgreSQL开发的
在使用上，kingbase更接近oracle的语法。
Kingbase支持更多的数值类型：decimal、numeric
底层结构都是B树
MySQL支持多种存储引擎，如InnoDB，MyISAM。Kingbase则采用自己的存储引擎



## redis
### redis介绍
2.Redis的特点
Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。

Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。

Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。

### redis应用场景
分布式缓存  
分布式锁    数据库并发插入，秒杀系统
计数器     网站浏览量，视频播放次数
轻量的消息队列
排行榜 zset
最新列表  list
存储session   博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。

### redis数据类型
Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。

String:
会采用预分配冗余空间的方式来减少内存的频繁分配，实际分配的空间 capacity 一般要高于实际字符串长度 len 
当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。

list 采用的存储结构是双向链表
在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。

Redis hash 是一个键值(key=>value)对集合。
hash 与 Java 中的 HashMap 差不多，实现上采用二维结构，第一维是数组，第二维是链表。hash 的 key 与 value 都存储在链表中，而数组中存储的则是各个链表的表头。在检索时，首先计算 key 的 hashcode，然后通过 hashcode 定位到链表的表头，再遍历链表得到 value 值。
初始化大小为4

set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。

参考<https://blog.51cto.com/u_14032861/2998549>

### redis hash
在Redis中，键值对（Key-Value Pair）存储方式是由字典（Dict）保存的，而字典底层是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。我们知道当HashMap中由于Hash冲突（负载因子）超过某个阈值时，出于链表性能的考虑，会进行Resize的操作。Redis也一样。

在redis的具体实现中，使用了一种叫做渐进式哈希(rehashing)的机制来提高字典的缩放效率，避免 rehash 对服务器性能造成影响，渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

扩容是扩容为原来的2倍，如果原来不是2的n次方
则是大于等于size的第一个2的n次方

size初始化为4
hash大小size同时也是扩容的阈值

dictht ht[2]：在字典内部，维护了两张哈希表。 一般情况下， 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用
```
typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    unsigned long iterators; /* number of iterators currently running */
} dict;

/* This is our hash table structure. Every dictionary has two of this as we
 * implement incremental rehashing, for the old to the new table. */
typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;

typedef struct dictEntry {
    void *key;                //键
    union {
        void *val;            //值
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next; //指向下一个节点，形成链表
} dictEntry;
```
扩容条件 
```
/*
     * 如果哈希表ht[0]中保存的key个数与哈希表大小的比例已经达到1:1，即保存的节点数已经大于哈希表大小
     * 且redis服务当前允许执行rehash，或者保存的节点数与哈希表大小的比例超过了安全阈值（默认值为5）
     * 则将哈希表大小扩容为原来的两倍
     */
 if (d->ht[0].used >= d->ht[0].size &&
        (dict_can_resize ||
         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio))
    {
        return dictExpand(d, d->ht[0].used*2);
    }
```

缩容
当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。
缩容后的大小为第一个大于等于当前key数量的2的n次方。最小容量为4。
同样从dictResize函数中可以看到，如果当前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，则不进行缩容

### 渐进式hash小结
在redis中，扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。

以下是哈希表渐进式 rehash 的详细步骤：

（1）为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。

（2）在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。

（3）在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。

（4）随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。

渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

### rehash的其他细节和缺点
渐进式 rehash 执行期间的哈希表操作
因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。

另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。

### 渐进式rehash带来的问题
渐进式rehash避免了redis阻塞，可以说非常完美，但是由于在rehash时，需要分配一个新的hash表，在rehash期间，同时有两个hash表在使用，会使得
使用量瞬间突增，在Redis 满容状态下由于Rehash会导致大量Key驱逐。

如果一个key一直不被访问，是不是rehash一直不结束？
答：不是的，redis有兜底策略，会在后台定期做一些迁移


### redis主从同步

### 单线程的redis为什么这么快
(一)纯内存操作
(二)单线程操作，避免了频繁的上下文切换
(三)采用了非阻塞I/O多路复用机制 （多路-指的是多个socket连接，复用-指的是复用一个线程，采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求。非阻塞指不会因为一个连接阻塞其他连接的请求）
(redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。)

>**多路复用原理**
在多路复用 I/O 模型中，会有一个线程不断去轮询多个 socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 I/O 读写操作。因为在多路复用 I/O 模型中，只需要使用一个线程就可以管理多个 socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程
> ，并且只有在真正有 socket 读写事件进行时，才会使用 I/O 资源，所以它大大减少了资源占用（如 CPU）

### redis为什么是单线程的
因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。

// todo 新版本redis是多线程了

### redis的四个问题
(一)缓存和数据库双写一致性问题
    正常流程：先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。
(二)缓存雪崩问题
(三)缓存穿透问题
(四)缓存的并发竞争问题

### redis删除策略
redis采用的是定期删除+惰性删除策略。
定期删除，redis默认每隔100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。
于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
采用定期删除+惰性删除就没其他问题了么?
不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制（详见下文）。

### redis内存淘汰策略
noeviction(默认策略)：对于写请求不再提供服务，直接返回错误（DEL请求和部分特殊请求除外）   
volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰  
volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰  
volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰  
allkeys-lru：（推荐）从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰  
allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰  
allkeys-lfu：对所有键值对使用 LFU（最不经常使用）策略进行淘汰。
no-enviction（驱逐）：禁止驱逐数据  

### lru，lfu原理
参考<https://zhuanlan.zhihu.com/p/363750871>

### 缓存穿透
缓存穿透是指查询一个一定不存在的数据（数据库也不存在），由于缓存的写入是在缓存中查不到，数据库中查到后被动写入，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。
***解决方案：***
(一)利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试
(二)采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。
(三)布隆过滤器

有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。
布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。

### 缓存雪崩
缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。
***解决方案：***
这里分享一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

### 缓存击穿
缓存击穿跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。
***解决方案：***
1. 将热点数据设置为永远不过期
2. 或者基于 redis or zookeeper 实现互斥锁，缓存失效后，等待失效后的第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。

### 缓存并发竞争问题
问题描述： 同时有多个客户端去set一个key。

解决办法：
(1)乐观锁，使用对修改顺序没有要求的场景
(1)如果对这个key操作，不要求顺序
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。
(2)如果对这个key操作，要求顺序
假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.
期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下

系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
那么，假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。

其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。 

### redis分布式锁是怎么实现的
使用setnx方法来争抢锁，抢到之后，再用expire方法给锁加一个过期时间防止锁忘记了释放。 
```
if (setnx(key, 1) == 1){
    expire(key, 30)
    try {
        //TODO 业务逻辑
    } finally {
        del(key)
    }
}
```

如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？
1.可以同时把setnx和expire合成一条指令来用    set key value nx ex 5
2.使用 lua 脚本

参考<https://juejin.cn/post/6855129006619459592>

### redis zset原理


### 超时解锁导致并发
如果线程 A 成功获取锁并设置过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁，线程 A 和线程 B 并发执行。

解决办法：
1.将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。
2.为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间。

### 锁误解除
如果线程 A 成功获取到了锁，并且设置了过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁；随后 A 执行完成，线程 A 使用 DEL 命令来释放锁，但此时线程 B 加的锁还没有执行完成，线程 A 实际释放的线程 B 加的锁。

解决办法：通过在 value 中设置当前线程加锁的标识，在删除之前验证 key 对应的 value 判断锁是否是当前线程持有。可生成一个 UUID 标识当前线程，使用 lua 脚本做验证标识和解锁操作。


### redis主从复制
为了保证 Redis 的可用性，一般采用主从方式部署。主从数据同步有异步和同步两种方式，Redis 将指令记录在本地内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一致的状态，一边向主节点反馈同步情况。

### redis缓存数据库双写一致性
// todo
有四种方案
1.更新数据库，更新缓存
2.更新缓存，更新数据库
3.删除缓存，更新数据库`
4.更新数据库，删除缓存

1. 1的问题在于 。  a更新数据库，b更新数据库，b更新缓存，a更新缓存，数据不一致 。如果缓存不是直接拿的数据库值，而是经过一个复杂的计算，则这样每次都重新计算下，但是又不怎么读这个数，太费资源。
2. 2的问题和1的问题一样。  a更新缓存，b更新缓存，b更新数据库，a更新数据库，导致双写不一致
3. 3的问题在于，a删了缓存，b读取数据，读了老数据库的数据，更新缓存，a再更新数据库，导致数据不一致。
   解决：a最后再删除缓存，这个叫延时双删
4. 正确的做法，但是还是有问题：更新数据库后，删除缓存失败。解决：删除缓存失败后将key值扔进消息队列，再从消息队列中取key来删除，问题：和业务代码耦合太深，解决:从binlog日志中读取对mysql的操作，然后将key扔进mq中再删除

参考<https://juejin.cn/post/6964531365643550751>

### 一致性hash 
todo

### redis需要掌握的
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_4.png)


## 待分类

### epoll原理 
socket概念：
在 Linux 操作系统中， Socket 是用来替代传输层以上协议实体的标准接口，它负责实现传输层以上所有的功能，可以说 Socket 是 TCP/IP 协议栈对外的窗口。Socket 是介于应用层与传输层中间的软件抽象层，它是一组接口。
如何创建Socket:
将Socket与地址绑定，设置Socket选项；
建立Socket之间的连接；
监听Socket；
接收、发送数据；
关闭、释放Socket；
参考<https://zhuanlan.zhihu.com/p/464268288>

常见io模型：
同步阻塞 IO(Blocking IO, BIO)
同步非阻塞IO(NIO)
IO 多路复用 （linux中：一个线程处理多个IO流。）
异步非阻塞 IO(Async IO, AIO)（其中AIO为异步IO，其他都是同步IO）

IO 多路复用有三种机制：
select、poll 和 epoll

epoll原理：
有3个接口：epoll_create（创建一个epoll的句柄），epoll_ctl（事件注册函数），epoll_wait（等待事件的产生）。
epoll实例内部维护了两个结构，分别是记录要监听的fd和已经就绪的fd，可以监听就绪的fd
查找就绪的文件描述符：epoll_wait返回就绪的文件描述符。
epoll在Linux内核中构建了一个文件系统，该文件系统采用红黑树来构建，红黑树在增加和删除上面的效率极高，因此是epoll高效的原因之一。

使用IO多路复用的技术框架
redis：Redis 的ae_select.c和ae_epoll.c文件，就分别使用了 select 和 epoll 这两种机制，实现 IO 多路复用；

nginx：Nginx支持epoll、select、kqueue等不同操作系统下的各种IO多路复用方式；Nginx是通过 ET模式使用 epoll。

>这边一定要看参考，我写的不详细

参考<https://zhuanlan.zhihu.com/p/462924941>
<https://zhuanlan.zhihu.com/p/427512269>
<https://juejin.cn/post/6882984260672847879>

### java接口架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_1.png)

### fork join  // todo


## mybatis
### mybatis 原理 // todo 
mybatis访问数据库有两种方式
1. mybatis api
创建一个和数据库打交道的SqlSession对象，然后根据Statement Id 和参数来操作数据库
2. mapper接口
通过动态代理机制生成一个Mapper 实例，我们使用Mapper接口的某一个方法时，MyBatis会根据这个方法的方法名和参数类型，确定Statement Id，底层还是通过SqlSession.select("statementId",parameterObject);或者SqlSession.update("statementId",parameterObject); 等等来实现对数据库的操作

原理
MyBatis启动时，解析mybatis的配置文件，并且从指定路径下解析mapper.xml配置文件 
把每条sql语句映射成MappedStatement
然后把MappedStatement存放到Configuration的一个mappedStatements属性中（mappedStatements是一个HashMap），key为namespace + id，value为MappedStatement
当要执行sql语句的时候，从mappedStatements这个map中通过id找到MappedStatement
获取MappedStatement对应sql语句、查询参数
查看一级缓存中有没有数据，有则直接返回
缓存没有数据，则查询数据库 
通过调用原生的jdbc方法，执行sql语句，获取到结果，删除旧缓存
把结果放到一级缓存，返回结果

### mybatis架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_2.png)

<https://blog.csdn.net/luanlouis/article/details/40422941>

<https://www.jianshu.com/p/ec40a82cae28>

// todo 找下mybatis的面试题


### mybatis缓存
### 一级缓存
默认开启：一级缓存是本地（局部）缓存，不能被关闭，只能配置缓存范围：SESSION 或 STATEMENT。
结论：不开启事务时，一级缓存不生效；开启了事务，一级缓存生效
原因：因为它每条语句执行结束以后，都会执行commit提交方法，而提交方法在每次都会清空本地缓存。而开启了事务的话，方法是在所有操作结束以后才会提交，因此就会支持一级缓存啦。一级缓存的生命周期是 SqlSession 对象的使用期间
当session内发生更新、删除、插入操作时，会清除当前session的一级缓存，但是不同session之间的缓存不会影响。即每个事务会生成自己的SqlSession缓存。一级缓存也只会在单个事务中的多次重复查询生效。
如果缓存级别是SESSION，事务隔离级别是读提交，A事务内两次查询之间，另外一个地方B更新了数据，则A的第二次查询不能查到更新后的数据，因为有一级缓存。解决办法，缓存范围改为STATEMENT。
参考<https://www.jianshu.com/p/fed09c106645>
### 二级缓存
默认情况下，mybatis打开了二级缓存，但它并未生效，因为二级缓存的作用域是namespace，所以还需要在Mapper.xml文件中配置一下才能使二级缓存生效


### Mybatis-spring-boot-starter自动配置的原理分析
mybatis-spring-boot-starter的作用就是，在SpringBoot启动时，去扫描所有Mapper接口，然后为其增加一个代理实现类，在调用的过程中，我们实际调用的是这个代理对象。

1. mybatis-spring-boot-starter将mybatis需要的依赖全部引入
2. starter同时通过SPI机制引入了一个配置Class：MybatisAutoConfiguration，它负责注册SqlSessionFactory和SqlSessionTemplate到Spring容器中，我们使用Mybatis时绝大部分功能靠这两个Bean实现
3. 引入了AutoConfiguredMapperScannerRegistrar这个bean到Spring容器，它负责将MapperScanner引入Spring容器，然后MapperScanner会将工程中所有的Dao扫描出来转换为BeanDefinition并且注册到Spring容器中
4. 当开发的工程中使用了某个Dao时，Spring能够从容器中找到这个Dao对应的BeanDefinition，将其实例化并且注入，这样开发者就可以使用了，这也是为何我们只定义了Dao的接口，但是工程运行时能够有实例Bean的原因
参考<https://www.cnblogs.com/nullifier/p/11967659.html>

## jvm
请看jvm.md

### g1垃圾收集器，强软弱虚引用，gc算法

### 内存泄漏怎么排查问题

### mybatis怎么写 like查询，即不同于普通的like查询

### jvm类加载机制 过程

### jvm内存模型


## todo
线程池怎么保证core线程数量不变
g1的原理，好处
分库分表是对 groupid这个字段进行hash，这有什么缺陷
synchronized无锁状态怎么实现的
es怎么提高查询效率
spring原理，动态代理原理，rpc框架动态代理原理
lru


## 平时整理
### 接口速度慢
原因分析：for循环，根据每个用户id去查询用户名。这种每次循环只查一个id，循环100次，速度很慢
修改：从业务逻辑层面修改，where id in()，一次性把需要的id name都查出来。
或者 人员表数据量不大的情况，把整个表的数据加载进来，用map存起来，然后for循环去map里查name



